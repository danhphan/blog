{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Self Attention from scratch\n",
    "\n",
    "> Implement Self Attention from scratch by using torch tensor. The goal is to understand how MultiHead Attention work."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate synthesis data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "# Using two fake languages: a and b\n",
    "copus_a = [\"one is one\", \"two is two\", \"three is three\", \"four is four\", \"five is five\",\n",
    "           \"six is six\", \"seven is seven\", \"eight is eight\", \"nine is nine\"]\n",
    "copus_b = [\"1 = 1\", \"2 = 2\", \"3 = 3\", \"4 = 4\", \"5 = 5\",\n",
    "           \"6 = 6\", \"7 = 7\", \"8 = 8\", \"9 = 9\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "# Using dumb 1-hot encoding for word_embedding\n",
    "embed_a = {\"one\":  [1.0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "           \"two\":  [0,1.0,0,0,0,0,0,0,0,0,0,0],\n",
    "           \"three\":[0,0,1.0,0,0,0,0,0,0,0,0,0],\n",
    "           \"four\": [0,0,0,1.0,0,0,0,0,0,0,0,0],\n",
    "           \"five\": [0,0,0,0,1.0,0,0,0,0,0,0,0],\n",
    "           \"six\":  [0,0,0,0,0,1.0,0,0,0,0,0,0],\n",
    "           \"seven\":[0,0,0,0,0,0,1.0,0,0,0,0,0],\n",
    "           \"eight\":[0,0,0,0,0,0,0,1.0,0,0,0,0],\n",
    "           \"nine\": [0,0,0,0,0,0,0,0,1.0,0,0,0],\n",
    "           \"is\":   [0,0,0,0,0,0,0,0,0,1.0,0,0],\n",
    "           \"less\": [0,0,0,0,0,0,0,0,0,0,1.0,0],\n",
    "           \"more\": [0,0,0,0,0,0,0,0,0,0,0,1.0]\n",
    "          }\n",
    "\n",
    "embed_b = {\"9\": [1.0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "           \"8\": [0,1.0,0,0,0,0,0,0,0,0,0,0],\n",
    "           \"7\": [0,0,1.0,0,0,0,0,0,0,0,0,0],\n",
    "           \"6\": [0,0,0,1.0,0,0,0,0,0,0,0,0],\n",
    "           \"5\": [0,0,0,0,1.0,0,0,0,0,0,0,0],\n",
    "           \"4\": [0,0,0,0,0,1.0,0,0,0,0,0,0],\n",
    "           \"3\": [0,0,0,0,0,0,1.0,0,0,0,0,0],\n",
    "           \"2\": [0,0,0,0,0,0,0,1.0,0,0,0,0],\n",
    "           \"1\": [0,0,0,0,0,0,0,0,1.0,0,0,0],\n",
    "           \"=\": [0,0,0,0,0,0,0,0,0,1.0,0,0],\n",
    "           \"<\": [0,0,0,0,0,0,0,0,0,1.0,0,0],\n",
    "           \">\": [0,0,0,0,0,0,0,0,0,1.0,0,0],\n",
    "          }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "def sentence_embed(sentence, embed_dict):\n",
    "    \"\"\"Generate an embedding for a sentence\"\"\"\n",
    "    res = []\n",
    "    for word in sentence.split():\n",
    "        res.append(embed_dict[word])\n",
    "    return res  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "# Create input and output sentenceinp\n",
    "inp = sentence_embed(\"one is one\", embed_a) \n",
    "out = sentence_embed(\"1 = 1\", embed_b)\n",
    "inp = torch.tensor(inp, dtype=torch.float32)\n",
    "out = torch.tensor(out, dtype=torch.float32)\n",
    "inp.shape, out.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([3, 12]), torch.Size([3, 12]))"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scaled dot product attention"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "def dot_attention(q, k, v):\n",
    "    \"\"\"inp: input sentence, dk: keyword dimension\"\"\"\n",
    "    # Initiate weight matrix for Query, Key and Value\n",
    "    dk = k.size(-1)\n",
    "    logit = (q @ k.transpose(0, -1)) / math.sqrt(dk)\n",
    "    weights = torch.softmax(logit, dim=-1)\n",
    "    res = weights @ v\n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "q, k, v = inp, inp, inp\n",
    "dot_attention(q, k, v)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.7275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2725, 0.0000, 0.0000],\n",
       "        [0.5998, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.4002, 0.0000, 0.0000],\n",
       "        [0.7275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2725, 0.0000, 0.0000]])"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-head Attention"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dm, nh):\n",
    "        \"\"\"\n",
    "        dm: model dimenstion\n",
    "        nh: number of heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dm, self.nh = dm, nh\n",
    "        self.dk = dm // nh\n",
    "        self.heads = [{\"wq\":nn.Linear(self.dm, self.dk),\n",
    "                      \"wk\":nn.Linear(self.dm, self.dk),\n",
    "                      \"wv\":nn.Linear(self.dm, self.dk)} for h in range(nh)\n",
    "                     ]        \n",
    "        self.out = nn.Linear(dm, dm)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        res = []\n",
    "        for head in self.heads:\n",
    "            q, k, v = head[\"wq\"](inp), head[\"wk\"](inp), head[\"wv\"](inp)\n",
    "            print(q.shape, k.shape, v.shape)\n",
    "            res.append(dot_attention(q, k, v))\n",
    "        concat = torch.cat(res, 1)\n",
    "        res = self.out(concat)\n",
    "        print(concat.shape, res.shape)\n",
    "        return res        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# Test\n",
    "dm = 12\n",
    "nh = 3\n",
    "# dk = 12/3 = 4\n",
    "mul_head = MultiHeadAttention(dm, nh)\n",
    "mul_head(inp)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4])\n",
      "torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4])\n",
      "torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4])\n",
      "torch.Size([3, 12]) torch.Size([3, 12])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.4953e-01,  6.1958e-02, -9.2505e-02,  1.4574e-01,  1.0211e-01,\n",
       "         -1.9842e-03,  8.9212e-02,  9.2313e-02, -2.3563e-01, -5.9226e-02,\n",
       "         -2.6632e-01, -1.9141e-01],\n",
       "        [-1.5497e-01,  6.4145e-02, -9.3638e-02,  1.4637e-01,  1.0329e-01,\n",
       "         -6.2585e-07,  9.0302e-02,  9.7207e-02, -2.3449e-01, -5.7356e-02,\n",
       "         -2.6794e-01, -1.9412e-01],\n",
       "        [-1.4953e-01,  6.1958e-02, -9.2505e-02,  1.4574e-01,  1.0211e-01,\n",
       "         -1.9842e-03,  8.9212e-02,  9.2313e-02, -2.3563e-01, -5.9226e-02,\n",
       "         -2.6632e-01, -1.9141e-01]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### References:\n",
    "* http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "* http://jalammar.github.io/illustrated-transformer/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}