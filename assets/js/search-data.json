{
  
    
        "post0": {
            "title": "A [maybe] better way to learn Gaussian Process [WIP]",
            "content": "1. Pre-knowledge . Before moving to Gaussian Process, a Bayesian non-parametric method, one should be familiar with parametric Bayesian models | Firstly, I will start from Richard McElreath&#39;s Statistical Rethinking by watching his lecture on Youtube, reading the book and doing excercises. The homework solution coded in PyMC is here thanks to Gabriel B.C. I prefer Python and PyMC, so I will use the PyMC implemetation of the book. | Secondly, Bayesian Analysis with Python (second edition) by Osvaldo Martin is a really good book to learn Bayseian data analysis with PyMC. | Thirdly, Probabilistic Programming and Bayesian Methods for Hackers: An introduction to Bayesian methods and probabilistic programming. This one really help to know how Bayesian methods are used in different applications. | . I also found the PyMCon2020 talk: My Journey in Learning and Relearning Bayesian Statistics by Ali Akbar Septiandri is really helpful. . 2. Gaussian Process [Once Gaussian always Gaussian] . 2.1. Kernels . The lecture video and notes on the Machine Learning for Intelligent Systems course at Cornell University is a great introduction on general kernels as as Linear, Polynomial, Radial Basis Function (RBF) (aka Gaussian Kernel), Exponential Kernel, ... . https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote14.html . Note that not any function K(⋅,⋅) → R can be used as a kernel. Only the matrix K(xi,xj) has to correspond to real inner-products after some transformation x→ϕ(x), and if and only if K is positive semi-definite. . Later, to learn more on kernels, . 2.2. Introduction to Gaussian Process . Chris Fonnesbeck: A Primer on Gaussian Processes for Regression Analysis | PyData NYC 2019, Youtube link Notebooks on Github link . Lecture 15: Gaussian Processes . 2.3. Deep dive into GP . On kernels: . Chapter 5, Carl Eduard Rasmussen and Christopher K.I. Williams, “Gaussian Processes for Machine Learning”, MIT Press 2006, the PDF version of the book here | The Kernel Cookbook: Advice on Covariance functions by David Duvenaud here | PyMC examples of GP: https://github.com/pymc-devs/pymc-examples/tree/main/examples/gaussian_processes | . Deep dive into GP by implementing GP from scratch . Small notes . At the begining, it is kind of difficult to understand and work with GP. It needs resilient. I have watched and re-watched some videos and played with notebooks several times. . Knowning GP helps understanding more on parametric Bayesian models and distributions. Expecially Multivariate Normal distribution. .",
            "url": "https://danhphan.github.io/blog/gp/gaussian%20process/2022/04/20/learn-gaussian-process.html",
            "relUrl": "/gp/gaussian%20process/2022/04/20/learn-gaussian-process.html",
            "date": " • Apr 20, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Gaussian Process from Scratch [WIP]",
            "content": "%matplotlib inline %config InlineBackend.figure_format = &#39;svg&#39; import numpy as np import scipy import matplotlib import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.axes_grid1 import make_axes_locatable import matplotlib.gridspec as gridspec import seaborn as sns # Set matplotlib and seaborn plotting style sns.set_style(&#39;darkgrid&#39;) np.random.seed(42) . 1. A kernel example . def exp_quadratic(xa, xb): &quot;&quot;&quot;Exponentiated quadratic with σ=1&quot;&quot;&quot; # L2 distance (Squared Euclidian) sq_norm = -0.5 * scipy.spatial.distance.cdist(xa, xb, &#39;sqeuclidean&#39;) return np.exp(sq_norm) . X = np.expand_dims(np.linspace(*xlim, 25), 1) Σ = exp_quadratic(X, X) plt.imshow(Σ, cmap=cm.YlGnBu); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-03-28T15:57:12.744982 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ zero = np.array([[0]]) Σ0 = exp_quadratic(X, zero) plt.plot(X[:,0], Σ0[:,0]); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-03-28T15:57:14.116661 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ 2. Sampling from prior . n_samples = 100 n_funcs = 8 X = np.expand_dims(np.linspace(-4,4, n_samples), 1) Σ = exp_quadratic(X, X) ys = np.random.multivariate_normal(mean=np.zeros(n_samples), cov=Σ, size=n_funcs) . for i in range(n_funcs): plt.plot(X, ys[i], linestyle=&#39;-&#39;, marker=&#39;o&#39;, markersize=3) plt.xlabel(&#39;$x$&#39;, fontsize=13) plt.ylabel(&#39;$y = f(x)$&#39;, fontsize=13) plt.title(( f&#39;{n_funcs} different function realizations at {n_samples} points n&#39; &#39;sampled from a Gaussian process with exponentiated quadratic kernel&#39;)) plt.xlim([-4, 4]) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-03-28T16:01:41.515158 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ exponentiated_quadratic = exp_quadratic . A = np.array([[1,-2j],[2j,5]]) A, A.shape . (array([[ 1.+0.j, -0.-2.j], [ 0.+2.j, 5.+0.j]]), (2, 2)) . Cholesky decomposition in numpy . L = np.linalg.cholesky(A) . np.dot(L, L.T.conj()) . array([[1.+0.j, 0.-2.j], [0.+2.j, 5.+0.j]]) . A = [[1,-2j],[2j,5]] # what happens if A is only array_like? np.linalg.cholesky(A) # an ndarray object is returned . array([[1.+0.j, 0.+0.j], [0.+2.j, 1.+0.j]]) . np.linalg.cholesky(np.matrix(A)) . matrix([[1.+0.j, 0.+0.j], [0.+2.j, 1.+0.j]]) . def GP(X1, y1, X2, kernel_func): cov11 = kernel_func(X1, X1) cov12 = kernel_func(X1, X2) solved = scipy.linalg.solve(cov11, cov12, assume_a=&#39;pos&#39;).T mu2 = solved @ y1 cov22 = kernel_func(X2, X2) cov2 = cov22 - (solved @ cov12) return mu2, cov2 . TODO: Convert K-1 by using cholesky . https://jaketae.github.io/study/gaussian-process/ . def GP2(X1, y1, X2, kernel_func): K11 = kernel_func(X1, X1) K12 = kernel_func(X1, X2) K22 = kernel_func(X2, X2) #L = np.linalg.cholesky(K11) mu2 = K12.T.dot(np.linalg.inv(K11)).dot(y1) cov2 = K22 - K12.T.dot(np.linalg.inv(K11)).dot(K12) return mu2, cov2 . ny = 10 # Number of functions domain = (-6, 6) domain[0]+2, domain[1]-2, (n1, 1) X1 = np.random.uniform(domain[0]+2, domain[1]-2, size=(n1, 1)) X1.shape . (50, 1) . %%prun f_sin = lambda x: (np.sin(x)).flatten() n1 = 40 # Train points n2 = 75 # Test points ny = 5 # Number of functions domain = (-6, 6) X1 = np.random.uniform(domain[0]+2, domain[1]-2, size=(n1, 1)) y1 = f_sin(X1) X2 = np.linspace(domain[0], domain[1], n2).reshape(-1, 1) # mu2, cov2 = GP(X1, y1, X2, exp_quadratic) mu2, cov2 = GP2(X1, y1, X2, exp_quadratic) sigma2 = np.sqrt(np.diag(cov2)) y2 = np.random.multivariate_normal(mean=mu2, cov=cov2, size=ny) . . &lt;string&gt;:16: RuntimeWarning: invalid value encountered in sqrt &lt;string&gt;:18: RuntimeWarning: covariance is not positive-semidefinite. . 296 function calls (287 primitive calls) in 0.044 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 2 0.040 0.020 0.040 0.020 linalg.py:476(inv) 1 0.001 0.001 0.001 0.001 linalg.py:1482(svd) 3 0.001 0.000 0.001 0.000 3828527866.py:1(exp_quadratic) 1 0.000 0.000 0.002 0.002 {method &#39;multivariate_normal&#39; of &#39;numpy.random.mtrand.RandomState&#39; objects} 3 0.000 0.000 0.000 0.000 {built-in method scipy.spatial._distance_pybind.cdist_sqeuclidean} 1 0.000 0.000 0.044 0.044 {built-in method builtins.exec} 17/8 0.000 0.000 0.042 0.005 {built-in method numpy.core._multiarray_umath.implement_array_function} 1 0.000 0.000 0.000 0.000 {method &#39;flatten&#39; of &#39;numpy.ndarray&#39; objects} 4 0.000 0.000 0.000 0.000 {method &#39;dot&#39; of &#39;numpy.ndarray&#39; objects} 1 0.000 0.000 0.044 0.044 &lt;string&gt;:1(&lt;module&gt;) 1 0.000 0.000 0.000 0.000 function_base.py:23(linspace) 3 0.000 0.000 0.000 0.000 socket.py:480(send) 1 0.000 0.000 0.000 0.000 &lt;__array_function__ internals&gt;:2(linspace) 1 0.000 0.000 0.000 0.000 {method &#39;uniform&#39; of &#39;numpy.random.mtrand.RandomState&#39; objects} 1 0.000 0.000 0.041 0.041 624826088.py:1(GP2) 1 0.000 0.000 0.000 0.000 numeric.py:2344(within_tol) 4 0.000 0.000 0.000 0.000 {method &#39;reduce&#39; of &#39;numpy.ufunc&#39; objects} 2 0.000 0.000 0.000 0.000 &lt;__array_function__ internals&gt;:2(result_type) 3 0.000 0.000 0.000 0.000 distance.py:2616(cdist) 6 0.000 0.000 0.000 0.000 {method &#39;astype&#39; of &#39;numpy.ndarray&#39; objects} 1 0.000 0.000 0.000 0.000 numeric.py:2264(isclose) 16 0.000 0.000 0.000 0.000 {built-in method numpy.array} 4 0.000 0.000 0.000 0.000 fromnumeric.py:70(_wrapreduction) 2 0.000 0.000 0.000 0.000 warnings.py:35(_formatwarnmsg_impl) 3 0.000 0.000 0.000 0.000 iostream.py:208(schedule) 2 0.000 0.000 0.000 0.000 iostream.py:502(write) 3 0.000 0.000 0.000 0.000 linalg.py:135(_commonType) 1 0.000 0.000 0.000 0.000 &lt;string&gt;:1(&lt;lambda&gt;) 2 0.000 0.000 0.000 0.000 _ufunc_config.py:32(seterr) 2 0.000 0.000 0.000 0.000 warnings.py:403(__init__) 2 0.000 0.000 0.000 0.000 {built-in method builtins.abs} 3 0.000 0.000 0.000 0.000 linalg.py:107(_makearray) 1 0.000 0.000 0.000 0.000 fromnumeric.py:1513(diagonal) 2 0.000 0.000 0.000 0.000 warnings.py:20(_showwarnmsg_impl) 2 0.000 0.000 0.000 0.000 linecache.py:82(updatecache) 2 0.000 0.000 0.000 0.000 _ufunc_config.py:132(geterr) 1 0.000 0.000 0.000 0.000 twodim_base.py:229(diag) 1 0.000 0.000 0.000 0.000 {built-in method numpy.arange} 3 0.000 0.000 0.000 0.000 fromnumeric.py:2355(all) 2 0.000 0.000 0.000 0.000 iostream.py:420(_is_master_process) 3 0.000 0.000 0.000 0.000 threading.py:1071(is_alive) 9 0.000 0.000 0.000 0.000 _asarray.py:23(asarray) 3 0.000 0.000 0.000 0.000 linalg.py:102(get_linalg_error_extobj) 6 0.000 0.000 0.000 0.000 _asarray.py:110(asanyarray) 2 0.000 0.000 0.040 0.020 &lt;__array_function__ internals&gt;:2(inv) 1 0.000 0.000 0.000 0.000 &lt;frozen importlib._bootstrap&gt;:1017(_handle_fromlist) 3 0.000 0.000 0.000 0.000 threading.py:1017(_wait_for_tstate_lock) 1 0.000 0.000 0.000 0.000 {method &#39;any&#39; of &#39;numpy.generic&#39; objects} 12 0.000 0.000 0.000 0.000 {built-in method builtins.len} 2 0.000 0.000 0.000 0.000 numerictypes.py:285(issubclass_) 4 0.000 0.000 0.000 0.000 fromnumeric.py:71(&lt;dictcomp&gt;) 2 0.000 0.000 0.000 0.000 &lt;__array_function__ internals&gt;:2(dot) 1 0.000 0.000 0.000 0.000 numeric.py:2186(allclose) 3 0.000 0.000 0.000 0.000 &lt;__array_function__ internals&gt;:2(all) 3 0.000 0.000 0.000 0.000 {method &#39;acquire&#39; of &#39;_thread.lock&#39; objects} 2 0.000 0.000 0.000 0.000 linecache.py:15(getline) 4 0.000 0.000 0.000 0.000 linalg.py:125(_realType) 3 0.000 0.000 0.000 0.000 linalg.py:193(_assert_stacked_2d) 12 0.000 0.000 0.000 0.000 {built-in method builtins.issubclass} 2 0.000 0.000 0.000 0.000 {method &#39;reshape&#39; of &#39;numpy.ndarray&#39; objects} 1 0.000 0.000 0.000 0.000 numerictypes.py:359(issubdtype) 1 0.000 0.000 0.000 0.000 fromnumeric.py:2256(any) 7 0.000 0.000 0.000 0.000 {method &#39;get&#39; of &#39;dict&#39; objects} 1 0.000 0.000 0.000 0.000 &lt;__array_function__ internals&gt;:2(allclose) 3 0.000 0.000 0.000 0.000 iostream.py:97(_event_pipe) 2 0.000 0.000 0.000 0.000 warnings.py:96(_showwarnmsg) 1 0.000 0.000 0.001 0.001 &lt;__array_function__ internals&gt;:2(svd) 8 0.000 0.000 0.000 0.000 {built-in method builtins.isinstance} 2 0.000 0.000 0.000 0.000 linecache.py:37(getlines) 1 0.000 0.000 0.000 0.000 &lt;__array_function__ internals&gt;:2(diag) 2 0.000 0.000 0.000 0.000 {built-in method posix.getpid} 6 0.000 0.000 0.000 0.000 linalg.py:112(isComplexType) 4 0.000 0.000 0.000 0.000 {built-in method builtins.getattr} 1 0.000 0.000 0.000 0.000 {method &#39;diagonal&#39; of &#39;numpy.ndarray&#39; objects} 2 0.000 0.000 0.000 0.000 linalg.py:199(_assert_stacked_square) 2 0.000 0.000 0.000 0.000 {built-in method numpy.seterrobj} 1 0.000 0.000 0.000 0.000 &lt;__array_function__ internals&gt;:2(diagonal) 1 0.000 0.000 0.000 0.000 _methods.py:53(_any) 2 0.000 0.000 0.000 0.000 iostream.py:439(_schedule_flush) 2 0.000 0.000 0.000 0.000 warnings.py:117(_formatwarnmsg) 1 0.000 0.000 0.000 0.000 numeric.py:1865(isscalar) 1 0.000 0.000 0.000 0.000 &lt;__array_function__ internals&gt;:2(any) 1 0.000 0.000 0.000 0.000 _ufunc_config.py:433(__enter__) 2 0.000 0.000 0.000 0.000 {method &#39;startswith&#39; of &#39;str&#39; objects} 1 0.000 0.000 0.000 0.000 &lt;__array_function__ internals&gt;:2(ndim) 1 0.000 0.000 0.000 0.000 _ufunc_config.py:438(__exit__) 4 0.000 0.000 0.000 0.000 {method &#39;__array_prepare__&#39; of &#39;numpy.ndarray&#39; objects} 4 0.000 0.000 0.000 0.000 {built-in method numpy.geterrobj} 1 0.000 0.000 0.000 0.000 &lt;__array_function__ internals&gt;:2(isclose) 1 0.000 0.000 0.000 0.000 fromnumeric.py:3106(ndim) 1 0.000 0.000 0.000 0.000 _ufunc_config.py:429(__init__) 3 0.000 0.000 0.000 0.000 {method &#39;lower&#39; of &#39;str&#39; objects} 4 0.000 0.000 0.000 0.000 {method &#39;items&#39; of &#39;dict&#39; objects} 3 0.000 0.000 0.000 0.000 threading.py:513(is_set) 1 0.000 0.000 0.000 0.000 {built-in method builtins.hasattr} 2 0.000 0.000 0.000 0.000 {method &#39;endswith&#39; of &#39;str&#39; objects} 2 0.000 0.000 0.000 0.000 linalg.py:472(_unary_dispatcher) 3 0.000 0.000 0.000 0.000 {method &#39;append&#39; of &#39;collections.deque&#39; objects} 3 0.000 0.000 0.000 0.000 fromnumeric.py:2350(_all_dispatcher) 2 0.000 0.000 0.000 0.000 multiarray.py:716(dot) 2 0.000 0.000 0.000 0.000 multiarray.py:644(result_type) 1 0.000 0.000 0.000 0.000 numeric.py:2182(_allclose_dispatcher) 3 0.000 0.000 0.000 0.000 {built-in method builtins.callable} 1 0.000 0.000 0.000 0.000 function_base.py:18(_linspace_dispatcher) 1 0.000 0.000 0.000 0.000 linalg.py:1478(_svd_dispatcher) 1 0.000 0.000 0.000 0.000 {built-in method _operator.index} 1 0.000 0.000 0.000 0.000 numeric.py:2260(_isclose_dispatcher) 1 0.000 0.000 0.000 0.000 fromnumeric.py:3102(_ndim_dispatcher) 1 0.000 0.000 0.000 0.000 {method &#39;disable&#39; of &#39;_lsprof.Profiler&#39; objects} 1 0.000 0.000 0.000 0.000 fromnumeric.py:2251(_any_dispatcher) 1 0.000 0.000 0.000 0.000 fromnumeric.py:1509(_diagonal_dispatcher) 1 0.000 0.000 0.000 0.000 twodim_base.py:225(_diag_dispatcher) . fig, (ax1, ax2) = plt.subplots( nrows=2, ncols=1, figsize=(6, 6)) # Plot the distribution of the function (mean, covariance) ax1.plot(X2, f_sin(X2), &#39;b--&#39;, label=&#39;$sin(x)$&#39;) ax1.fill_between(X2.flat, mu2-2*sigma2, mu2+2*sigma2, color=&#39;red&#39;, alpha=0.15, label=&#39;$2 sigma_{2|1}$&#39;) ax1.plot(X2, mu2, &#39;r-&#39;, lw=2, label=&#39;$ mu_{2|1}$&#39;) ax1.plot(X1, y1, &#39;ko&#39;, linewidth=2, label=&#39;$(x_1, y_1)$&#39;) # Plot some samples from this function ax2.plot(X2, y2.T, &#39;-&#39;) ax2.set_xlabel(&#39;$x$&#39;, fontsize=13) ax2.set_ylabel(&#39;$y$&#39;, fontsize=13) ax2.set_title(&#39;5 different function realizations from posterior&#39;) ax1.axis([domain[0], domain[1], -3, 3]) ax2.set_xlim([-6, 6]) plt.tight_layout() plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-03-28T17:15:07.852920 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/",
            "url": "https://danhphan.github.io/blog/gp/numpy/2022/04/05/gp-from-scratch.html",
            "relUrl": "/gp/numpy/2022/04/05/gp-from-scratch.html",
            "date": " • Apr 5, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Transformers from HuggingFace :)",
            "content": "from transformers import pipeline . Sentiment analysis . classifier = pipeline(&#39;sentiment-analysis&#39;) . classifier(&quot;Ihave waiting for a course my whole life.&quot;) . [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9817631840705872}] . alist = [&quot;Covid is good&quot;, &quot;I love covid&quot;] classifier(alist) . [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998581409454346}, {&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.999816358089447}] . Zero-shot classification . classifier = pipeline(&quot;zero-shot-classification&quot;) . classifier(&quot;This is a sensitive topic on transport and libarary&quot;, candidate_labels=[&quot;education&quot;, &quot;math&quot;, &quot;business&quot;]) . {&#39;sequence&#39;: &#39;This is a sensitive topic on transport and libarary&#39;, &#39;labels&#39;: [&#39;business&#39;, &#39;education&#39;, &#39;math&#39;], &#39;scores&#39;: [0.5921958684921265, 0.22866113483905792, 0.17914298176765442]} . Text generation . generator = pipeline(&quot;text-generation&quot;) . generator(&quot;In this notebook, we will&quot;) . Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. . [{&#39;generated_text&#39;: &#39;In this notebook, we will create several templates to illustrate how to generate the JavaScript code. For simplicity, we used the Angular JS example from one of our previous posts. n nIn fact, we would not think you would not understand. The above&#39;}] . gen_gpt2 = pipeline(&quot;text-generation&quot;, model=&quot;distilgpt2&quot;) gen_gpt2(&quot;In this pandas notebook, we will&quot;, max_lenght=20) . Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. . [{&#39;generated_text&#39;: &#34;In this pandas notebook, we will be using the first part of the first part of the series on Pandas as a learning tool for students to learn from Pandas. In the next post, we&#39;ll look at how to use both learning tools&#34;}] . Mask filling . unmasker = pipeline(&quot;fill-mask&quot;) unmasker(&quot;This notebook will show &lt;mask&gt; direction&quot;, top_k=2) . [{&#39;sequence&#39;: &#39;This notebook will show visual direction&#39;, &#39;score&#39;: 0.05728420987725258, &#39;token&#39;: 7133, &#39;token_str&#39;: &#39; visual&#39;}, {&#39;sequence&#39;: &#39;This notebook will show editorial direction&#39;, &#39;score&#39;: 0.053508173674345016, &#39;token&#39;: 8161, &#39;token_str&#39;: &#39; editorial&#39;}] . Named entity recognition . ner = pipeline(&quot;ner&quot;, grouped_entities=True) ner(&quot;I am Dan P who carry out research at Monash Uni in Melbourne City&quot;) . /home/danph/.pyenv/versions/3.8.5/envs/.ml/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:154: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=&#34;AggregationStrategy.SIMPLE&#34;` instead. warnings.warn( . [{&#39;entity_group&#39;: &#39;PER&#39;, &#39;score&#39;: 0.9955268, &#39;word&#39;: &#39;Dan P&#39;, &#39;start&#39;: 5, &#39;end&#39;: 10}, {&#39;entity_group&#39;: &#39;ORG&#39;, &#39;score&#39;: 0.991117, &#39;word&#39;: &#39;Monash Uni&#39;, &#39;start&#39;: 37, &#39;end&#39;: 47}, {&#39;entity_group&#39;: &#39;LOC&#39;, &#39;score&#39;: 0.971781, &#39;word&#39;: &#39;Melbourne City&#39;, &#39;start&#39;: 51, &#39;end&#39;: 65}] . Question answering . qa = pipeline(&quot;question-answering&quot;) qa(question=&quot;Where to I work&quot;, context=&quot;I am Dan P who carry out research at Monash Uni in Melbourne City&quot;) . {&#39;score&#39;: 0.8566566705703735, &#39;start&#39;: 37, &#39;end&#39;: 47, &#39;answer&#39;: &#39;Monash Uni&#39;} . Summarization . summarizer = pipeline(&quot;summarization&quot;) . article = &quot;&quot;&quot; The ongoing discussions within the presidential palace in Kabul are “utterly extraordinary” after two decades of war, CNN International Security Editor Nick Paton Walsh reports. “This has been a morning of stunning events and that looks like we are heading towards some sort of transitional government here,” Paton Walsh said. He said names are being floated around, though nothing is confirmed, and President Ashraf Ghani would need to agree to step aside to make way for a transitional administration. Yesterday Ghani made a brief but sombre address to the nation in which he said he was consulting with elders and other leaders both inside and outside of the country. In the short speech, he told the Afghan people his &quot;focus is to avoid further instability, aggression and displacement,&quot; but he did not resign. As talks on Sunday continue, Paton Walsh said there hasn’t been evidence of Taliban fighters moving into the city. Earlier panic appeared to be a clash around a bank where people were trying to withdraw money. “I&#39;ve heard sporadic gunfire here but that seems to be traffic disputes. A quick drive around the city has shown traffic has dissipated until you get towards the airport, so utter chaos and panic here. The traffic in the skies we saw around the embassy appears to have quietened as well so perhaps that might suggest some of that operation is winding up,” he continued. The apparently last-ditch diplomatic efforts would hopefully avoid the Taliban presumably moving to its next phase of slowly entering the city, which Paton Walsh said would “not be remotely pleasant for anybody living here.” “There will be elements of resistance too so I think everybody would prefer to avoid that kind of situation,” he added. &quot;&quot;&quot; . summarizer(article) . /home/danph/.pyenv/versions/3.8.5/envs/.ml/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the &#39;trunc&#39; function NOT &#39;floor&#39;). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode=&#39;trunc&#39;), or for actual floor division, use torch.div(a, b, rounding_mode=&#39;floor&#39;). (Triggered internally at /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.) return torch.floor_divide(self, other) . [{&#39;summary_text&#39;: &#39; President Ashraf Ghani made a brief but sombre address to the nation in which he said he was consulting with elders and other leaders both inside and outside of the country . CNN International Security Editor Nick Paton Walsh said there hasn’t been evidence of Taliban fighters moving into the city .&#39;}] . Translation . translator = pipeline(&quot;translation&quot;, model=&quot;t5-base&quot;) . /home/danph/.pyenv/versions/3.8.5/envs/.ml/lib/python3.8/site-packages/transformers/pipelines/__init__.py:497: UserWarning: &#34;translation&#34; task was used, instead of &#34;translation_XX_to_YY&#34;, defaulting to &#34;translation_en_to_de&#34; warnings.warn( . translator(&quot;Hi, my name is Dan&quot;) . [{&#39;translation_text&#39;: &#39;Hallo, mein Name ist Dan&#39;}] .",
            "url": "https://danhphan.github.io/blog/jupyter/huggingface/transformer/2021/10/21/transfomers.html",
            "relUrl": "/jupyter/huggingface/transformer/2021/10/21/transfomers.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Self Attention from scratch",
            "content": "import math import numpy as np import torch import torch.nn as nn . Generate synthesis data . copus_a = [&quot;one is one&quot;, &quot;two is two&quot;, &quot;three is three&quot;, &quot;four is four&quot;, &quot;five is five&quot;, &quot;six is six&quot;, &quot;seven is seven&quot;, &quot;eight is eight&quot;, &quot;nine is nine&quot;] copus_b = [&quot;1 = 1&quot;, &quot;2 = 2&quot;, &quot;3 = 3&quot;, &quot;4 = 4&quot;, &quot;5 = 5&quot;, &quot;6 = 6&quot;, &quot;7 = 7&quot;, &quot;8 = 8&quot;, &quot;9 = 9&quot;] . embed_a = {&quot;one&quot;: [1.0,0,0,0,0,0,0,0,0,0,0,0], &quot;two&quot;: [0,1.0,0,0,0,0,0,0,0,0,0,0], &quot;three&quot;:[0,0,1.0,0,0,0,0,0,0,0,0,0], &quot;four&quot;: [0,0,0,1.0,0,0,0,0,0,0,0,0], &quot;five&quot;: [0,0,0,0,1.0,0,0,0,0,0,0,0], &quot;six&quot;: [0,0,0,0,0,1.0,0,0,0,0,0,0], &quot;seven&quot;:[0,0,0,0,0,0,1.0,0,0,0,0,0], &quot;eight&quot;:[0,0,0,0,0,0,0,1.0,0,0,0,0], &quot;nine&quot;: [0,0,0,0,0,0,0,0,1.0,0,0,0], &quot;is&quot;: [0,0,0,0,0,0,0,0,0,1.0,0,0], &quot;less&quot;: [0,0,0,0,0,0,0,0,0,0,1.0,0], &quot;more&quot;: [0,0,0,0,0,0,0,0,0,0,0,1.0] } embed_b = {&quot;9&quot;: [1.0,0,0,0,0,0,0,0,0,0,0,0], &quot;8&quot;: [0,1.0,0,0,0,0,0,0,0,0,0,0], &quot;7&quot;: [0,0,1.0,0,0,0,0,0,0,0,0,0], &quot;6&quot;: [0,0,0,1.0,0,0,0,0,0,0,0,0], &quot;5&quot;: [0,0,0,0,1.0,0,0,0,0,0,0,0], &quot;4&quot;: [0,0,0,0,0,1.0,0,0,0,0,0,0], &quot;3&quot;: [0,0,0,0,0,0,1.0,0,0,0,0,0], &quot;2&quot;: [0,0,0,0,0,0,0,1.0,0,0,0,0], &quot;1&quot;: [0,0,0,0,0,0,0,0,1.0,0,0,0], &quot;=&quot;: [0,0,0,0,0,0,0,0,0,1.0,0,0], &quot;&lt;&quot;: [0,0,0,0,0,0,0,0,0,1.0,0,0], &quot;&gt;&quot;: [0,0,0,0,0,0,0,0,0,1.0,0,0], } . def sentence_embed(sentence, embed_dict): &quot;&quot;&quot;Generate an embedding for a sentence&quot;&quot;&quot; res = [] for word in sentence.split(): res.append(embed_dict[word]) return res . inp = sentence_embed(&quot;one is one&quot;, embed_a) out = sentence_embed(&quot;1 = 1&quot;, embed_b) inp = torch.tensor(inp, dtype=torch.float32) out = torch.tensor(out, dtype=torch.float32) inp.shape, out.shape . (torch.Size([3, 12]), torch.Size([3, 12])) . Scaled dot product attention . def dot_attention(q, k, v): &quot;&quot;&quot;inp: input sentence, dk: keyword dimension&quot;&quot;&quot; # Initiate weight matrix for Query, Key and Value dk = k.size(-1) logit = (q @ k.transpose(0, -1)) / math.sqrt(dk) weights = torch.softmax(logit, dim=-1) res = weights @ v return res . q, k, v = inp, inp, inp dot_attention(q, k, v) . tensor([[0.7275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2725, 0.0000, 0.0000], [0.5998, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4002, 0.0000, 0.0000], [0.7275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2725, 0.0000, 0.0000]]) . Multi-head Attention . class MultiHeadAttention(nn.Module): def __init__(self, dm, nh): &quot;&quot;&quot; dm: model dimenstion nh: number of heads &quot;&quot;&quot; super().__init__() self.dm, self.nh = dm, nh self.dk = dm // nh self.heads = [{&quot;wq&quot;:nn.Linear(self.dm, self.dk), &quot;wk&quot;:nn.Linear(self.dm, self.dk), &quot;wv&quot;:nn.Linear(self.dm, self.dk)} for h in range(nh) ] self.out = nn.Linear(dm, dm) def forward(self, inp): res = [] for head in self.heads: q, k, v = head[&quot;wq&quot;](inp), head[&quot;wk&quot;](inp), head[&quot;wv&quot;](inp) print(q.shape, k.shape, v.shape) res.append(dot_attention(q, k, v)) concat = torch.cat(res, 1) res = self.out(concat) print(concat.shape, res.shape) return res . dm = 12 nh = 3 # dk = 12/3 = 4 mul_head = MultiHeadAttention(dm, nh) mul_head(inp) . torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 4]) torch.Size([3, 12]) torch.Size([3, 12]) . tensor([[-1.4953e-01, 6.1958e-02, -9.2505e-02, 1.4574e-01, 1.0211e-01, -1.9842e-03, 8.9212e-02, 9.2313e-02, -2.3563e-01, -5.9226e-02, -2.6632e-01, -1.9141e-01], [-1.5497e-01, 6.4145e-02, -9.3638e-02, 1.4637e-01, 1.0329e-01, -6.2585e-07, 9.0302e-02, 9.7207e-02, -2.3449e-01, -5.7356e-02, -2.6794e-01, -1.9412e-01], [-1.4953e-01, 6.1958e-02, -9.2505e-02, 1.4574e-01, 1.0211e-01, -1.9842e-03, 8.9212e-02, 9.2313e-02, -2.3563e-01, -5.9226e-02, -2.6632e-01, -1.9141e-01]], grad_fn=&lt;AddmmBackward&gt;) . References: . http://nlp.seas.harvard.edu/2018/04/03/attention.html | http://jalammar.github.io/illustrated-transformer/ | .",
            "url": "https://danhphan.github.io/blog/jupyter/selfattention/transformer/2021/09/15/attention.html",
            "relUrl": "/jupyter/selfattention/transformer/2021/09/15/attention.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Encoding categorical features",
            "content": "Encoding Categorical Data . Ordinal Encoding . # define data data = asarray([[&#39;red&#39;], [&#39;green&#39;], [&#39;blue&#39;]]) print(data) # define ordinal encoding encoder = OrdinalEncoder() # transform data result = encoder.fit_transform(data) print(result) . [[&#39;red&#39;] [&#39;green&#39;] [&#39;blue&#39;]] [[2.] [1.] [0.]] . One-Hot Encoding . from numpy import asarray from sklearn.preprocessing import OneHotEncoder # define data data = asarray([[&#39;red&#39;], [&#39;green&#39;], [&#39;blue&#39;]]) print(data) # define one hot encoding encoder = OneHotEncoder(sparse=False) # transform data onehot = encoder.fit_transform(data) print(onehot) . [[&#39;red&#39;] [&#39;green&#39;] [&#39;blue&#39;]] [[0. 0. 1.] [0. 1. 0.] [1. 0. 0.]] . Dummy Variable Encoding . from numpy import asarray from sklearn.preprocessing import OneHotEncoder # define data data = asarray([[&#39;red&#39;], [&#39;green&#39;], [&#39;blue&#39;]]) print(data) # define one hot encoding encoder = OneHotEncoder(drop=&#39;first&#39;, sparse=False) # transform data onehot = encoder.fit_transform(data) print(onehot) . [[&#39;red&#39;] [&#39;green&#39;] [&#39;blue&#39;]] [[0. 1.] [1. 0.] [0. 0.]] . Categorical Encoding example . from pandas import read_csv from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OrdinalEncoder # define the location of the dataset url = &quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv&quot; # load the dataset dataset = read_csv(url, header=None) # retrieve the array of data data = dataset.values # separate into input and output columns X = data[:, :-1].astype(str) y = data[:, -1].astype(str) # summarize print(&#39;Input&#39;, X.shape) print(&#39;Output&#39;, y.shape) . Input (286, 9) Output (286,) . dataset.head() . 0 1 2 3 4 5 6 7 8 9 . 0 &#39;40-49&#39; | &#39;premeno&#39; | &#39;15-19&#39; | &#39;0-2&#39; | &#39;yes&#39; | &#39;3&#39; | &#39;right&#39; | &#39;left_up&#39; | &#39;no&#39; | &#39;recurrence-events&#39; | . 1 &#39;50-59&#39; | &#39;ge40&#39; | &#39;15-19&#39; | &#39;0-2&#39; | &#39;no&#39; | &#39;1&#39; | &#39;right&#39; | &#39;central&#39; | &#39;no&#39; | &#39;no-recurrence-events&#39; | . 2 &#39;50-59&#39; | &#39;ge40&#39; | &#39;35-39&#39; | &#39;0-2&#39; | &#39;no&#39; | &#39;2&#39; | &#39;left&#39; | &#39;left_low&#39; | &#39;no&#39; | &#39;recurrence-events&#39; | . 3 &#39;40-49&#39; | &#39;premeno&#39; | &#39;35-39&#39; | &#39;0-2&#39; | &#39;yes&#39; | &#39;3&#39; | &#39;right&#39; | &#39;left_low&#39; | &#39;yes&#39; | &#39;no-recurrence-events&#39; | . 4 &#39;40-49&#39; | &#39;premeno&#39; | &#39;30-34&#39; | &#39;3-5&#39; | &#39;yes&#39; | &#39;2&#39; | &#39;left&#39; | &#39;right_up&#39; | &#39;no&#39; | &#39;recurrence-events&#39; | . type(dataset), type(data), type(X), type(y) . (pandas.core.frame.DataFrame, numpy.ndarray, numpy.ndarray, numpy.ndarray) . y.shape, X.shape, data.shape, dataset.shape . ((286,), (286, 9), (286, 10), (286, 10)) . OrdinalEncoder Transform . ordinal_encoder = OrdinalEncoder() X = ordinal_encoder.fit_transform(X) # ordinal encode target variable label_encoder = LabelEncoder() y = label_encoder.fit_transform(y) # summarize the transformed data print(&#39;Input&#39;, X.shape) print(X[:5, :]) print(&#39;Output&#39;, y.shape) print(y[:5]) . Input (286, 9) [[2. 2. 2. 0. 1. 2. 1. 2. 0.] [3. 0. 2. 0. 0. 0. 1. 0. 0.] [3. 0. 6. 0. 0. 1. 0. 1. 0.] [2. 2. 6. 0. 1. 2. 1. 1. 1.] [2. 2. 5. 4. 1. 1. 0. 4. 0.]] Output (286,) [1 0 1 0 1] . from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.metrics import accuracy_score . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # ordinal encode input variables ordinal_encoder = OrdinalEncoder() ordinal_encoder.fit(X_train) X_train = ordinal_encoder.transform(X_train) X_test = ordinal_encoder.transform(X_test) # ordinal encode target variable label_encoder = LabelEncoder() label_encoder.fit(y_train) y_train = label_encoder.transform(y_train) y_test = label_encoder.transform(y_test) # define the model model = LogisticRegression() # fit on the training set model.fit(X_train, y_train) # predict on test set yhat = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print(&#39;Accuracy: %.2f&#39; % (accuracy*100)) . Accuracy: 75.79 . OneHotEncoder Transform . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # one-hot encode input variables onehot_encoder = OneHotEncoder() onehot_encoder.fit(X_train) X_train = onehot_encoder.transform(X_train) X_test = onehot_encoder.transform(X_test) # ordinal encode target variable label_encoder = LabelEncoder() label_encoder.fit(y_train) y_train = label_encoder.transform(y_train) y_test = label_encoder.transform(y_test) # define the model model = LogisticRegression() # fit on the training set model.fit(X_train, y_train) # predict on test set yhat = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print(&#39;Accuracy: %.2f&#39; % (accuracy*100)) . Accuracy: 70.53 .",
            "url": "https://danhphan.github.io/blog/2020/06/18/python_category_encoding.html",
            "relUrl": "/2020/06/18/python_category_encoding.html",
            "date": " • Jun 18, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Diferent clustering methods for spatial data",
            "content": "MeanShift . import numpy as np from sklearn.cluster import MeanShift, estimate_bandwidth from sklearn.datasets import make_blobs . centers = [[1, 1], [-1, -1], [1, -1]] X, _ = make_blobs(n_samples=60000, centers=centers, cluster_std=0.6) . X.shape, X . ((60000, 2), array([[ 1.19957726, 1.13094037], [-1.11365716, -2.56431276], [-1.10611655, -0.45558829], ..., [ 1.01367638, 1.54773387], [-1.4012213 , -0.99219442], [-0.88836584, -1.33994179]])) . bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500) bandwidth . 1.0237498469238382 . ms = MeanShift(bandwidth=bandwidth, bin_seeding=True) ms.fit(X) labels = ms.labels_ cluster_centers = ms.cluster_centers_ labels_unique = np.unique(labels) n_clusters_ = len(labels_unique) print(&quot;number of estimated clusters : %d&quot; % n_clusters_) . number of estimated clusters : 3 . cluster_centers . array([[ 0.92926946, -0.92318283], [ 0.99509878, 0.92338456], [-0.90341301, -0.99368369]]) . # Plot result import matplotlib.pyplot as plt from itertools import cycle plt.figure(1) plt.clf() colors = cycle(&#39;bgrcmykbgrcmykbgrcmykbgrcmyk&#39;) for k, col in zip(range(n_clusters_), colors): my_members = labels == k cluster_center = cluster_centers[k] plt.plot(X[my_members, 0], X[my_members, 1], col + &#39;.&#39;) plt.plot(cluster_center[0], cluster_center[1], &#39;o&#39;, markerfacecolor=col, markeredgecolor=&#39;k&#39;, markersize=14) plt.title(&#39;Estimated number of clusters: %d&#39; % n_clusters_) plt.show() . Next time need to test this: https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py .",
            "url": "https://danhphan.github.io/blog/2020/02/05/space_clustering.html",
            "relUrl": "/2020/02/05/space_clustering.html",
            "date": " • Feb 5, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Traffic networks with google elevation api",
            "content": "import os import numpy as np import geopandas as gpd import osmnx as ox . ELEVATION_API = os.environ.get(&#39;GCP_ELEVATION_API&#39;) . Get a network and add elevation to its nodes . GMB = ox.graph_from_place(&quot;Greater Melbourne, Victoria, Australia&quot;, network_type=&#39;all&#39;) len(GMB.nodes), len(GMB.edges) . (380268, 960778) . GM = ox.graph_from_place(&quot;City of Monash, Victoria, Australia&quot;, network_type=&#39;all&#39;) len(GM.nodes), len(GM.edges) . (13602, 32640) . Get a network by place . place = &quot;Oakleigh&quot; place_query = &quot;Oakleigh, the city of Monash, Victoria, 3166, Australia&quot; G = ox.graph_from_place(place_query, network_type=&#39;bike&#39;) . len(G.nodes), len(G.edges) . (611, 1431) . fig, ax = ox.plot_graph(G) . Get elevation data . G = ox.add_node_elevations(G, api_key=ELEVATION_API) G = ox.add_edge_grades(G) . Calculate several stats . Average and median grade . edge_grades = [data[&#39;grade_abs&#39;] for u, v, k, data in ox.get_undirected(G).edges(keys=True, data=True)] . avg_grade = np.mean(edge_grades) print(&#39;Average street grade in {} is {:.1f}%&#39;.format(place, avg_grade*100)) . Average street grade in Oakleigh is 1.9% . med_grade = np.median(edge_grades) print(&#39;Median street grade in {} is {:.1f}%&#39;.format(place, med_grade*100)) . Median street grade in Oakleigh is 1.4% . Plot nodes by elevation . nc = ox.plot.get_node_colors_by_attr(G, &#39;elevation&#39;, cmap=&#39;plasma&#39;) fig, ax = ox.plot_graph(G, node_color=nc, node_size=5, edge_color=&#39;#333333&#39;, bgcolor=&#39;k&#39;) . Plot the edges by grade . ec = ox.plot.get_edge_colors_by_attr(G, &#39;grade_abs&#39;, cmap=&#39;plasma&#39;, num_bins=5, equal_size=True) fig, ax = ox.plot_graph(G, edge_color=ec, edge_linewidth=0.5, node_size=0, bgcolor=&#39;k&#39;) . Shortest paths account for grade impedance . from shapely.geometry import Polygon, Point . orig = (-37.8943, 145.0900) dest = (-37.9059, 145.1030) # Check the distance Point(orig).distance(Point(dest)) . 0.017422973339822574 . orig = ox.get_nearest_node(G, orig) dest = ox.get_nearest_node(G, dest) bbox = ox.utils_geo.bbox_from_point((-37.9001, 145.0965), dist=1500) . def impedance(length, grade): penalty = grade ** 2 return length * penalty def impedance_2(length, grade): penalty = length * (np.abs(grade) ** 3) return penalty . for u, v, k, data in G.edges(keys=True, data=True): # data[&#39;impedance&#39;] = impedance(data[&#39;length&#39;], data[&#39;grade_abs&#39;]) data[&#39;impedance&#39;] = impedance_2(data[&#39;length&#39;], data[&#39;grade&#39;]) data[&#39;rise&#39;] = data[&#39;length&#39;] * data[&#39;grade&#39;] . First find the shortest path by minimising distance . route_by_length = ox.shortest_path(G, orig, dest, weight=&#39;length&#39;) fig, ax = ox.plot_graph_route(G, route_by_length, node_size=0) . Find the shortest path by minimising impedance . route_by_impedance = ox.shortest_path(G, orig, dest, weight=&#39;impedance&#39;) fig, ax = ox.plot_graph_route(G, route_by_impedance, node_size=0) . Stats about these two routes . def print_route_stats(route): route_grades = ox.utils_graph.get_route_edge_attributes(G, route, &#39;grade_abs&#39;) msg = &#39;The average grade is {:.1f}% and the max is {:.1f}%&#39; print(msg.format(np.mean(route_grades)*100, np.max(route_grades)*100)) route_rises = ox.utils_graph.get_route_edge_attributes(G, route, &#39;rise&#39;) ascent = np.sum([rise for rise in route_rises if rise &gt;= 0]) descent = np.sum([rise for rise in route_rises if rise &lt; 0]) msg = &#39;Total elevation change is {:.1f} meters: a {:.0f} meter ascent and a {:.0f} meter descent&#39; print(msg.format(np.sum(route_rises), ascent, abs(descent))) route_lengths = ox.utils_graph.get_route_edge_attributes(G, route, &#39;length&#39;) print(&#39;Total trip distance: {:,.0f} meters&#39;.format(np.sum(route_lengths))) . print_route_stats(route_by_length) . The average grade is 1.5% and the max is 5.4% Total elevation change is 12.3 meters: a 19 meter ascent and a 7 meter descent Total trip distance: 2,273 meters . print_route_stats(route_by_impedance) . The average grade is 1.1% and the max is 5.4% Total elevation change is 12.4 meters: a 22 meter ascent and a 10 meter descent Total trip distance: 3,423 meters .",
            "url": "https://danhphan.github.io/blog/elevation/googlemap/api/2020/01/25/gcp_elevation_api.html",
            "relUrl": "/elevation/googlemap/api/2020/01/25/gcp_elevation_api.html",
            "date": " • Jan 25, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Statsmodels introduction",
            "content": "Start model introduction . Loading modules and functions . Loading data . df = sm.datasets.get_rdataset(&quot;Guerry&quot;, &quot;HistData&quot;) type(df), type(df.data) . (statsmodels.datasets.utils.Dataset, pandas.core.frame.DataFrame) . df = df.data df.head() . dept Region Department Crime_pers Crime_prop Literacy Donations Infants Suicides MainCity ... Crime_parents Infanticide Donation_clergy Lottery Desertion Instruction Prostitutes Distance Area Pop1831 . 0 1 | E | Ain | 28870 | 15890 | 37 | 5098 | 33120 | 35039 | 2:Med | ... | 71 | 60 | 69 | 41 | 55 | 46 | 13 | 218.372 | 5762 | 346.03 | . 1 2 | N | Aisne | 26226 | 5521 | 51 | 8901 | 14572 | 12831 | 2:Med | ... | 4 | 82 | 36 | 38 | 82 | 24 | 327 | 65.945 | 7369 | 513.00 | . 2 3 | C | Allier | 26747 | 7925 | 13 | 10973 | 17044 | 114121 | 2:Med | ... | 46 | 42 | 76 | 66 | 16 | 85 | 34 | 161.927 | 7340 | 298.26 | . 3 4 | E | Basses-Alpes | 12935 | 7289 | 46 | 2733 | 23018 | 14238 | 1:Sm | ... | 70 | 12 | 37 | 80 | 32 | 29 | 2 | 351.399 | 6925 | 155.90 | . 4 5 | E | Hautes-Alpes | 17488 | 8174 | 69 | 6962 | 23076 | 16171 | 1:Sm | ... | 22 | 23 | 64 | 79 | 35 | 7 | 1 | 320.280 | 5549 | 129.10 | . 5 rows × 23 columns . vars = [&#39;Department&#39;, &#39;Lottery&#39;, &#39;Literacy&#39;, &#39;Wealth&#39;, &#39;Region&#39;] df = df[vars] df.shape . (86, 5) . df[-5:] . Department Lottery Literacy Wealth Region . 81 Vienne | 40 | 25 | 68 | W | . 82 Haute-Vienne | 55 | 13 | 67 | C | . 83 Vosges | 14 | 62 | 82 | E | . 84 Yonne | 51 | 47 | 30 | C | . 85 Corse | 83 | 49 | 37 | NaN | . df.isna().sum() . Department 0 Lottery 0 Literacy 0 Wealth 0 Region 1 dtype: int64 . df = df.dropna() df.shape, df.dtypes . ((85, 5), Department object Lottery int64 Literacy int64 Wealth int64 Region object dtype: object) . df[&#39;Region&#39;].value_counts() . S 17 E 17 N 17 W 17 C 17 Name: Region, dtype: int64 . Designed matrices and model . y, X = patsy.dmatrices(&#39;Lottery ~ Literacy + Wealth + Region&#39;, data=df, return_type=&#39;dataframe&#39;) y[:3], X[:3] . ( Lottery 0 41.0 1 38.0 2 66.0, Intercept Region[T.E] Region[T.N] Region[T.S] Region[T.W] Literacy 0 1.0 1.0 0.0 0.0 0.0 37.0 1 1.0 0.0 1.0 0.0 0.0 51.0 2 1.0 0.0 0.0 0.0 0.0 13.0 Wealth 0 73.0 1 22.0 2 61.0 ) . mod = sm.OLS(y, X) # Describe model res = mod.fit() . Results . res.summary() . OLS Regression Results Dep. Variable: Lottery | R-squared: 0.338 | . Model: OLS | Adj. R-squared: 0.287 | . Method: Least Squares | F-statistic: 6.636 | . Date: Sat, 02 Jan 2021 | Prob (F-statistic): 1.07e-05 | . Time: 23:45:18 | Log-Likelihood: -375.30 | . No. Observations: 85 | AIC: 764.6 | . Df Residuals: 78 | BIC: 781.7 | . Df Model: 6 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . Intercept 38.6517 | 9.456 | 4.087 | 0.000 | 19.826 | 57.478 | . Region[T.E] -15.4278 | 9.727 | -1.586 | 0.117 | -34.793 | 3.938 | . Region[T.N] -10.0170 | 9.260 | -1.082 | 0.283 | -28.453 | 8.419 | . Region[T.S] -4.5483 | 7.279 | -0.625 | 0.534 | -19.039 | 9.943 | . Region[T.W] -10.0913 | 7.196 | -1.402 | 0.165 | -24.418 | 4.235 | . Literacy -0.1858 | 0.210 | -0.886 | 0.378 | -0.603 | 0.232 | . Wealth 0.4515 | 0.103 | 4.390 | 0.000 | 0.247 | 0.656 | . Omnibus: 3.049 | Durbin-Watson: 1.785 | . Prob(Omnibus): 0.218 | Jarque-Bera (JB): 2.694 | . Skew: -0.340 | Prob(JB): 0.260 | . Kurtosis: 2.454 | Cond. No. 371. | . Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. res.params . Intercept 38.651655 Region[T.E] -15.427785 Region[T.N] -10.016961 Region[T.S] -4.548257 Region[T.W] -10.091276 Literacy -0.185819 Wealth 0.451475 dtype: float64 . res.rsquared . 0.3379508691928822 . sm.stats.linear_rainbow(res) . (0.847233997615691, 0.6997965543621644) . sm.graphics.plot_partregress(&#39;Lottery&#39;, &#39;Wealth&#39;, [&#39;Region&#39;, &#39;Literacy&#39;], ....: data=df, obs_labels=False) . Discrete choice basic with statsmodels . spector_data = sm.datasets.spector.load(as_pandas=False) spector_data.exog = sm.add_constant(spector_data.exog, prepend=False) print(spector_data.exog[:5,:]) print(spector_data.endog[:5]) . [[ 2.66 20. 0. 1. ] [ 2.89 22. 0. 1. ] [ 3.28 24. 0. 1. ] [ 2.92 12. 0. 1. ] [ 4. 21. 0. 1. ]] [0. 0. 0. 0. 1.] . df = sm.datasets.spector.load(as_pandas=True).data df[:5], type(df) . ( GPA TUCE PSI GRADE 0 2.66 20.0 0.0 0.0 1 2.89 22.0 0.0 0.0 2 3.28 24.0 0.0 0.0 3 2.92 12.0 0.0 0.0 4 4.00 21.0 0.0 1.0, pandas.core.frame.DataFrame) . Linear Probability Model (OLS) . lpm_mod = sm.OLS(spector_data.endog, spector_data.exog) lpm_res = lpm_mod.fit() print(&#39;Parameters: &#39;, lpm_res.params[:-1]) . Parameters: [0.46385168 0.01049512 0.37855479] . y, X = patsy.dmatrices(&#39;GRADE ~ GPA + TUCE + PSI&#39;, df) lpm2 = sm.OLS(y, X) res2 = lpm2.fit() res2.params[1:] . array([0.46385168, 0.01049512, 0.37855479]) . def test_eq(pred, targ): diff = np.sum(pred - targ) assert diff &lt; 1e-10 . test_eq(lpm_res.params[:-1], res2.params[1:]) . Logit Model . logit_mod = sm.Logit(spector_data.endog, spector_data.exog) logit_res = logit_mod.fit(disp=0) print(&#39;Parameters: &#39;, logit_res.params) . Parameters: [ 2.82611259 0.09515766 2.37868766 -13.02134686] . logit2 = sm.Logit(y,X) res2 = logit2.fit() res2.params . Optimization terminated successfully. Current function value: 0.402801 Iterations 7 . array([-13.02134686, 2.82611259, 0.09515766, 2.37868766]) . test_eq(logit_res.params[:-1], res2.params[1:]) . # logit3 = sm.Logit(&#39;GRADE ~ GPA + TUCE + PSI&#39;, df) . marg = res2.get_margeff() marg.summary() . Logit Marginal Effects Dep. Variable: GRADE | . Method: dydx | . At: overall | . dy/dx std err z P&gt;|z| [0.025 0.975] . GPA 0.3626 | 0.109 | 3.313 | 0.001 | 0.148 | 0.577 | . TUCE 0.0122 | 0.018 | 0.686 | 0.493 | -0.023 | 0.047 | . PSI 0.3052 | 0.092 | 3.304 | 0.001 | 0.124 | 0.486 | . Probit Model . probit_mod = sm.Probit(spector_data.endog, spector_data.exog) probit_res = probit_mod.fit() probit_margeff = probit_res.get_margeff() print(&#39;Parameters: &#39;, probit_res.params) print(&#39;Marginal effects: &#39;) print(probit_margeff.summary()) . Optimization terminated successfully. Current function value: 0.400588 Iterations 6 Parameters: [ 1.62581004 0.05172895 1.42633234 -7.45231965] Marginal effects: Probit Marginal Effects ===================================== Dep. Variable: y Method: dydx At: overall ============================================================================== dy/dx std err z P&gt;|z| [0.025 0.975] x1 0.3608 0.113 3.182 0.001 0.139 0.583 x2 0.0115 0.018 0.624 0.533 -0.025 0.048 x3 0.3165 0.090 3.508 0.000 0.140 0.493 ============================================================================== . Multinomial Logit . anes_data = sm.datasets.anes96.load(as_pandas=False) anes_exog = anes_data.exog anes_exog = sm.add_constant(anes_exog, prepend=False) print(anes_data.exog[:5,:]) print(anes_data.endog[:5]) . [[-2.30258509 7. 36. 3. 1. ] [ 5.24755025 3. 20. 4. 1. ] [ 3.43720782 2. 24. 6. 1. ] [ 4.4200447 3. 28. 6. 1. ] [ 6.46162441 5. 68. 6. 1. ]] [6. 1. 1. 1. 0.] . mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog) mlogit_res = mlogit_mod.fit() print(mlogit_res.params) . Optimization terminated successfully. Current function value: 1.548647 Iterations 7 [[-1.15359746e-02 -8.87506530e-02 -1.05966699e-01 -9.15567017e-02 -9.32846040e-02 -1.40880692e-01] [ 2.97714352e-01 3.91668642e-01 5.73450508e-01 1.27877179e+00 1.34696165e+00 2.07008014e+00] [-2.49449954e-02 -2.28978371e-02 -1.48512069e-02 -8.68134503e-03 -1.79040689e-02 -9.43264870e-03] [ 8.24914421e-02 1.81042758e-01 -7.15241904e-03 1.99827955e-01 2.16938850e-01 3.21925702e-01] [ 5.19655317e-03 4.78739761e-02 5.75751595e-02 8.44983753e-02 8.09584122e-02 1.08894083e-01] [-3.73401677e-01 -2.25091318e+00 -3.66558353e+00 -7.61384309e+00 -7.06047825e+00 -1.21057509e+01]] . mlogit_res.summary() . MNLogit Regression Results Dep. Variable: y | No. Observations: 944 | . Model: MNLogit | Df Residuals: 908 | . Method: MLE | Df Model: 30 | . Date: Sat, 02 Jan 2021 | Pseudo R-squ.: 0.1648 | . Time: 23:45:25 | Log-Likelihood: -1461.9 | . converged: True | LL-Null: -1750.3 | . Covariance Type: nonrobust | LLR p-value: 1.822e-102 | . y=1 coef std err z P&gt;|z| [0.025 0.975] . x1 -0.0115 | 0.034 | -0.336 | 0.736 | -0.079 | 0.056 | . x2 0.2977 | 0.094 | 3.180 | 0.001 | 0.114 | 0.481 | . x3 -0.0249 | 0.007 | -3.823 | 0.000 | -0.038 | -0.012 | . x4 0.0825 | 0.074 | 1.121 | 0.262 | -0.062 | 0.227 | . x5 0.0052 | 0.018 | 0.295 | 0.768 | -0.029 | 0.040 | . const -0.3734 | 0.630 | -0.593 | 0.553 | -1.608 | 0.861 | . y=2 coef std err z P&gt;|z| [0.025 0.975] . x1 -0.0888 | 0.039 | -2.266 | 0.023 | -0.166 | -0.012 | . x2 0.3917 | 0.108 | 3.619 | 0.000 | 0.180 | 0.604 | . x3 -0.0229 | 0.008 | -2.893 | 0.004 | -0.038 | -0.007 | . x4 0.1810 | 0.085 | 2.123 | 0.034 | 0.014 | 0.348 | . x5 0.0479 | 0.022 | 2.149 | 0.032 | 0.004 | 0.092 | . const -2.2509 | 0.763 | -2.949 | 0.003 | -3.747 | -0.755 | . y=3 coef std err z P&gt;|z| [0.025 0.975] . x1 -0.1060 | 0.057 | -1.858 | 0.063 | -0.218 | 0.006 | . x2 0.5735 | 0.159 | 3.617 | 0.000 | 0.263 | 0.884 | . x3 -0.0149 | 0.011 | -1.311 | 0.190 | -0.037 | 0.007 | . x4 -0.0072 | 0.126 | -0.057 | 0.955 | -0.255 | 0.240 | . x5 0.0576 | 0.034 | 1.713 | 0.087 | -0.008 | 0.123 | . const -3.6656 | 1.157 | -3.169 | 0.002 | -5.932 | -1.399 | . y=4 coef std err z P&gt;|z| [0.025 0.975] . x1 -0.0916 | 0.044 | -2.091 | 0.037 | -0.177 | -0.006 | . x2 1.2788 | 0.129 | 9.921 | 0.000 | 1.026 | 1.531 | . x3 -0.0087 | 0.008 | -1.031 | 0.302 | -0.025 | 0.008 | . x4 0.1998 | 0.094 | 2.123 | 0.034 | 0.015 | 0.384 | . x5 0.0845 | 0.026 | 3.226 | 0.001 | 0.033 | 0.136 | . const -7.6138 | 0.958 | -7.951 | 0.000 | -9.491 | -5.737 | . y=5 coef std err z P&gt;|z| [0.025 0.975] . x1 -0.0933 | 0.039 | -2.371 | 0.018 | -0.170 | -0.016 | . x2 1.3470 | 0.117 | 11.494 | 0.000 | 1.117 | 1.577 | . x3 -0.0179 | 0.008 | -2.352 | 0.019 | -0.033 | -0.003 | . x4 0.2169 | 0.085 | 2.552 | 0.011 | 0.050 | 0.384 | . x5 0.0810 | 0.023 | 3.524 | 0.000 | 0.036 | 0.126 | . const -7.0605 | 0.844 | -8.362 | 0.000 | -8.715 | -5.406 | . y=6 coef std err z P&gt;|z| [0.025 0.975] . x1 -0.1409 | 0.042 | -3.343 | 0.001 | -0.223 | -0.058 | . x2 2.0701 | 0.143 | 14.435 | 0.000 | 1.789 | 2.351 | . x3 -0.0094 | 0.008 | -1.160 | 0.246 | -0.025 | 0.007 | . x4 0.3219 | 0.091 | 3.534 | 0.000 | 0.143 | 0.500 | . x5 0.1089 | 0.025 | 4.304 | 0.000 | 0.059 | 0.158 | . const -12.1058 | 1.060 | -11.421 | 0.000 | -14.183 | -10.028 | . Logit model . dta = sm.datasets.fair.load_pandas().data . dta[&#39;affair&#39;] = (dta[&#39;affairs&#39;] &gt; 0).astype(float) print(dta.head(10)) . rate_marriage age yrs_married children religious educ occupation 0 3.0 32.0 9.0 3.0 3.0 17.0 2.0 1 3.0 27.0 13.0 3.0 1.0 14.0 3.0 2 4.0 22.0 2.5 0.0 1.0 16.0 3.0 3 4.0 37.0 16.5 4.0 3.0 16.0 5.0 4 5.0 27.0 9.0 1.0 1.0 14.0 3.0 5 4.0 27.0 9.0 0.0 2.0 14.0 3.0 6 5.0 37.0 23.0 5.5 2.0 12.0 5.0 7 5.0 37.0 23.0 5.5 2.0 12.0 2.0 8 3.0 22.0 2.5 0.0 2.0 12.0 3.0 9 3.0 27.0 6.0 0.0 1.0 16.0 3.0 occupation_husb affairs affair 0 5.0 0.111111 1.0 1 4.0 3.230769 1.0 2 5.0 1.400000 1.0 3 5.0 0.727273 1.0 4 4.0 4.666666 1.0 5 4.0 4.666666 1.0 6 4.0 0.852174 1.0 7 3.0 1.826086 1.0 8 3.0 4.799999 1.0 9 5.0 1.333333 1.0 . affair_mod = logit(&quot;affair ~ occupation + educ + occupation_husb&quot; &quot;+ rate_marriage + age + yrs_married + children&quot; &quot; + religious&quot;, dta).fit() . Optimization terminated successfully. Current function value: 0.545314 Iterations 6 . affair_mod.summary() . Logit Regression Results Dep. Variable: affair | No. Observations: 6366 | . Model: Logit | Df Residuals: 6357 | . Method: MLE | Df Model: 8 | . Date: Sat, 02 Jan 2021 | Pseudo R-squ.: 0.1327 | . Time: 23:45:26 | Log-Likelihood: -3471.5 | . converged: True | LL-Null: -4002.5 | . Covariance Type: nonrobust | LLR p-value: 5.807e-224 | . | coef std err z P&gt;|z| [0.025 0.975] . Intercept 3.7257 | 0.299 | 12.470 | 0.000 | 3.140 | 4.311 | . occupation 0.1602 | 0.034 | 4.717 | 0.000 | 0.094 | 0.227 | . educ -0.0392 | 0.015 | -2.533 | 0.011 | -0.070 | -0.009 | . occupation_husb 0.0124 | 0.023 | 0.541 | 0.589 | -0.033 | 0.057 | . rate_marriage -0.7161 | 0.031 | -22.784 | 0.000 | -0.778 | -0.655 | . age -0.0605 | 0.010 | -5.885 | 0.000 | -0.081 | -0.040 | . yrs_married 0.1100 | 0.011 | 10.054 | 0.000 | 0.089 | 0.131 | . children -0.0042 | 0.032 | -0.134 | 0.893 | -0.066 | 0.058 | . religious -0.3752 | 0.035 | -10.792 | 0.000 | -0.443 | -0.307 | . dta.shape, dta.affair.value_counts() . ((6366, 10), 0.0 4313 1.0 2053 Name: affair, dtype: int64) . def acc(cfmtx): return (cfmtx[0,0] + cfmtx[1,1])/np.sum(cfmtx) #How well are we predicting? affair_mod.pred_table(), acc(affair_mod.pred_table()) . (array([[3882., 431.], [1326., 727.]]), 0.7240025133521835) . mfx = affair_mod.get_margeff() mfx.summary() . Logit Marginal Effects Dep. Variable: affair | . Method: dydx | . At: overall | . dy/dx std err z P&gt;|z| [0.025 0.975] . occupation 0.0293 | 0.006 | 4.744 | 0.000 | 0.017 | 0.041 | . educ -0.0072 | 0.003 | -2.538 | 0.011 | -0.013 | -0.002 | . occupation_husb 0.0023 | 0.004 | 0.541 | 0.589 | -0.006 | 0.010 | . rate_marriage -0.1308 | 0.005 | -26.891 | 0.000 | -0.140 | -0.121 | . age -0.0110 | 0.002 | -5.937 | 0.000 | -0.015 | -0.007 | . yrs_married 0.0201 | 0.002 | 10.327 | 0.000 | 0.016 | 0.024 | . children -0.0008 | 0.006 | -0.134 | 0.893 | -0.012 | 0.011 | . religious -0.0685 | 0.006 | -11.119 | 0.000 | -0.081 | -0.056 | . Logit vs Probit . fig = plt.figure(figsize=(12,8)) ax = fig.add_subplot(111) support = np.linspace(-6, 6, 1000) ax.plot(support, stats.logistic.cdf(support), &#39;r-&#39;, label=&#39;Logistic&#39;) ax.plot(support, stats.norm.cdf(support), label=&#39;Probit&#39;) ax.legend(); . fig = plt.figure(figsize=(12,8)) ax = fig.add_subplot(111) support = np.linspace(-6, 6, 1000) ax.plot(support, stats.logistic.pdf(support), &#39;r-&#39;, label=&#39;Logistic&#39;) ax.plot(support, stats.norm.pdf(support), label=&#39;Probit&#39;) ax.legend(); .",
            "url": "https://danhphan.github.io/blog/jupyter/discrete%20choice/statsmodels/2019/07/12/statsmodels-intro.html",
            "relUrl": "/jupyter/discrete%20choice/statsmodels/2019/07/12/statsmodels-intro.html",
            "date": " • Jul 12, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Collecting Melbourne housing price by Beautiful Soup",
            "content": "In this tutorial we will scrap housing price in Melbourne from this web site: https://www.domain.com.au/auction-results/melbourne/ . Required python packages . We need urllib for connect to the website, and BeautifulSoup for parsing html sources. After that, the data can be stored in a json file. . from urllib.request import urlopen from urllib.error import HTTPError from bs4 import BeautifulSoup import json import pandas as pd import matplotlib import matplotlib.pyplot as plt . def getListings(url): &quot;Get a list of auction results&quot; try: html = urlopen(url) except HTTPError as e: return None try: # Parsing data bs = BeautifulSoup(html.read(), &#39;html.parser&#39;) articles = bs.findAll(&#39;article&#39;, {&#39;class&#39;:&#39;css-3xqrp1&#39;}) for atc in articles: for c in atc.children: if c.name == &#39;header&#39;: suburb = c.h3.text if c.name == &#39;ul&#39;: getListing(c, suburb) except AttributeError as e: return None def getListing(tag, suburb=None): &quot; Get a list in each suburb &quot; ladd,lagen,htype,hInfo,soldInfo,price = tuple([&quot;Unknown&quot; for i in range(6)]) listing = list(tag.children) ladd = listing[0].text if listing[1].name == &#39;li&#39;: htype,hInfo = getHouseInfo(listing[1]) if listing[2].name == &#39;li&#39;: soldInfo,price = getSoldInfo(listing[2]) if listing[3].name == &#39;li&#39;: lagen = listing[3].text listings.append( {&#39;suburb&#39;:suburb, &#39;street&#39;:ladd, &#39;agent&#39;:lagen, &#39;type&#39;: htype, &#39;info&#39;: hInfo, &#39;sold&#39;:soldInfo, &#39;price&#39;:price}) def getSoldInfo(tag): sold = list(tag.children) if len(sold) &gt;= 2: return sold[0].text, sold[1].text else: return sold[0].text, &quot;Unknown&quot; def getHouseInfo(tag): house = list(tag.children) if len(house) &gt;= 2: return house[0].text, house[1].text else: return house[0].text, &quot;Unknown&quot; . listings = [] # Store all listings url = &quot;https://www.domain.com.au/auction-results/melbourne/&quot; getListings(url) . Check results . len(listings) . 637 . listings[1:5] . [{&#39;suburb&#39;: &#39;Abbotsford&#39;, &#39;street&#39;: &#39;1/47 Nicholson St&#39;, &#39;agent&#39;: &#39;Biggin &amp; Scott Richmond&#39;, &#39;type&#39;: &#39;Townhouse&#39;, &#39;info&#39;: &#39;2 beds&#39;, &#39;sold&#39;: &#39;Sold prior to auction&#39;, &#39;price&#39;: &#39;$1.12m&#39;}, {&#39;suburb&#39;: &#39;Abbotsford&#39;, &#39;street&#39;: &#39;12 Paterson St&#39;, &#39;agent&#39;: &#39;Biggin &amp; Scott Richmond&#39;, &#39;type&#39;: &#39;House&#39;, &#39;info&#39;: &#39;4 beds&#39;, &#39;sold&#39;: &#39;Sold&#39;, &#39;price&#39;: &#39;$1.886m&#39;}, {&#39;suburb&#39;: &#39;Abbotsford&#39;, &#39;street&#39;: &#39;4 Turner St&#39;, &#39;agent&#39;: &#39;Jellis Craig Fitzroy&#39;, &#39;type&#39;: &#39;House&#39;, &#39;info&#39;: &#39;3 beds&#39;, &#39;sold&#39;: &#39;Sold&#39;, &#39;price&#39;: &#39;Price withheld&#39;}, {&#39;suburb&#39;: &#39;Airport West&#39;, &#39;street&#39;: &#39;2/74 Fraser St&#39;, &#39;agent&#39;: &#39;Barry Plant Essendon&#39;, &#39;type&#39;: &#39;Unit&#39;, &#39;info&#39;: &#39;3 beds&#39;, &#39;sold&#39;: &#39;Sold prior to auction&#39;, &#39;price&#39;: &#39;$671.5k&#39;}] . Store results . with open(&#39;listings.json&#39;, &#39;w&#39;) as f: json.dump(listings, f) . df = pd.read_json(&#39;listings.json&#39;) . df.shape . (637, 7) . df.head() . suburb street agent type info sold price . 0 Abbotsford | 34 Albert St | Unknown | Unknown | Unknown | House | | . 1 Abbotsford | 1/47 Nicholson St | Biggin &amp; Scott Richmond | Townhouse | 2 beds | Sold prior to auction | $1.12m | . 2 Abbotsford | 12 Paterson St | Biggin &amp; Scott Richmond | House | 4 beds | Sold | $1.886m | . 3 Abbotsford | 4 Turner St | Jellis Craig Fitzroy | House | 3 beds | Sold | Price withheld | . 4 Airport West | 2/74 Fraser St | Barry Plant Essendon | Unit | 3 beds | Sold prior to auction | $671.5k | . df[&#39;type&#39;].value_counts().plot(kind=&#39;bar&#39;) . &lt;AxesSubplot:&gt; . df[&#39;info&#39;].value_counts().plot(kind=&#39;bar&#39;) . &lt;AxesSubplot:&gt; .",
            "url": "https://danhphan.github.io/blog/jupyter/beautifulsoup/web%20scraping/2019/06/10/house-price-scraper.html",
            "relUrl": "/jupyter/beautifulsoup/web%20scraping/2019/06/10/house-price-scraper.html",
            "date": " • Jun 10, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Math optimisation with Scipy",
            "content": "import numpy as np from scipy import optimize . Scalar optimisation . def f(x): return -np.exp(-(x-0.7)**2) . result = optimize.minimize_scalar(f) . result.success, type(result) . (True, scipy.optimize.optimize.OptimizeResult) . result . fun: -1.0 nfev: 13 nit: 9 success: True x: 0.6999999997839409 . result.x - 0.7 . -2.160590595323697e-10 . def f(x): # The rosenbrock function return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2 . optimize.minimize(f, [2, -1], method=&quot;CG&quot;) . fun: 1.6503729082243953e-11 jac: array([-6.15347610e-06, 2.53804028e-07]) message: &#39;Optimization terminated successfully.&#39; nfev: 81 nit: 13 njev: 27 status: 0 success: True x: array([0.99999426, 0.99998863]) . def jacobian(x): return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2))) . optimize.minimize(f, [2, 1], method=&quot;CG&quot;, jac=jacobian) . fun: 2.957865890641887e-14 jac: array([ 7.18259502e-07, -2.99030306e-07]) message: &#39;Optimization terminated successfully.&#39; nfev: 16 nit: 8 njev: 16 status: 0 success: True x: array([1.00000012, 1.00000009]) . def f(x): # The rosenbrock function return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2 def jacobian(x): return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2))) optimize.minimize(f, [2,-1], method=&quot;Newton-CG&quot;, jac=jacobian) . fun: 1.5601357400786612e-15 jac: array([ 1.05753092e-07, -7.48325277e-08]) message: &#39;Optimization terminated successfully.&#39; nfev: 11 nhev: 0 nit: 10 njev: 33 status: 0 success: True x: array([0.99999995, 0.99999988]) . def hessian(x): # Computed with sympy return np.array(((1 - 4*x[1] + 12*x[0]**2, -4*x[0]), (-4*x[0], 2))) optimize.minimize(f, [2,-1], method=&quot;Newton-CG&quot;, jac=jacobian, hess=hessian) . fun: 1.6277298383706738e-15 jac: array([ 1.11044158e-07, -7.78093352e-08]) message: &#39;Optimization terminated successfully.&#39; nfev: 11 nhev: 10 nit: 10 njev: 11 status: 0 success: True x: array([0.99999994, 0.99999988]) .",
            "url": "https://danhphan.github.io/blog/2019/04/21/math_optimisation.html",
            "relUrl": "/2019/04/21/math_optimisation.html",
            "date": " • Apr 21, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Openstreetmap point of interest data",
            "content": "import pandas as pd import geopandas as gpd import osmnx as ox import matplotlib.pyplot as plt import contextily as cx . Get osm shop types . shop_types = pd.read_csv(&quot;../data/osm_shop_types.csv&quot;) shop_types.shape . (169, 5) . shop_types.head() . Key Value Comment Group Name Group . 0 shop | alcohol | Shop selling alcohol to take away | Food, beverages | Food | . 1 shop | bakery | Shop focused on selling bread | Food, beverages | Food | . 2 shop | beverages | Shop focused on selling alcoholic and non-alco... | Food, beverages | Food | . 3 shop | brewing_supplies | Shop focused on selling supplies for home brew... | Food, beverages | Food | . 4 shop | butcher | Shop focused on selling meat | Food, beverages | Food | . def plot_shops(place): tags = {&#39;building&#39;:True} bds = ox.geometries_from_place(place, tags) tags = {&#39;amenity&#39;:True} ams = ox.geometries_from_place(place, tags) tags = {&quot;shop&quot;:True} shs = ox.geometries_from_place(place, tags) f, ax = plt.subplots(1, figsize=(20, 20)) bds.plot(marker=1, color=&#39;black&#39;, ax=ax) ams.plot(ax=ax, color=&#39;b&#39;, marker=1, alpha=0.7) shs.plot(ax=ax, alpha=0.8, color=&#39;r&#39;, marker=1) cx.add_basemap(ax, crs=bds.crs.to_string(), source=cx.providers.Stamen.TonerLite) ax.set_axis_off() plt.show() . Shops in City of Monash, Victoria, Australia . place = &quot;City of Monash, Victoria, Australia&quot; tags = {&quot;shop&quot;:True, &quot;opening_hours&quot;:&quot;*&quot;} shops = ox.geometries_from_place(place, tags) shops.shape . (590, 69) . shops.shop.value_counts().head(50) . convenience 59 car_repair 41 hairdresser 39 supermarket 36 bakery 31 car 24 alcohol 24 beauty 22 butcher 18 clothes 17 travel_agency 13 massage 12 computer 11 mall 11 yes 11 greengrocer 10 doityourself 9 department_store 9 optician 9 florist 7 electronics 7 laundry 7 funeral_directors 6 gift 6 variety_store 6 newsagent 6 furniture 6 sports 6 tobacco 5 mobile_phone 4 car_parts 4 deli 4 second_hand 4 jewelry 4 pet 3 stationery 3 dry_cleaning 3 music 3 pawnbroker 3 shoes 3 tattoo 3 houseware 3 garden_centre 3 lottery 3 pastry 3 copyshop 2 books 2 wholesale 2 tailor 2 seafood 2 Name: shop, dtype: int64 . plc = &quot;City of Monash, Victoria, Australia&quot; plot_shops(plc) . Shops in Melbourne District, Melbourne, Victoria, Australia . plc = &quot;Melbourne District, Melbourne, City of Melbourne, Victoria, Australia&quot; plot_shops(plc) . /opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . Shops in Victoria, Australia . place = &quot;Victoria, Australia&quot; tags = {&quot;shop&quot;:True, &quot;opening_hours&quot;:&quot;*&quot;} shops = ox.geometries_from_place(place, tags) shops.shape . (14794, 369) . shops.shape, type(shops) . ((14794, 374), geopandas.geodataframe.GeoDataFrame) . columns = [&#39;unique_id&#39;, &#39;osmid&#39;, &#39;element_type&#39;, &#39;addr:city&#39;, &#39;addr:housenumber&#39;, &#39;addr:postcode&#39;, &#39;addr:state&#39;, &#39;addr:street&#39;, &#39;postal_code&#39;, &#39;name&#39;, &#39;opening_hours&#39;,&#39;opening_date&#39;, &#39;operator&#39;, &#39;phone&#39;, &#39;shop&#39;, &#39;source&#39;, &#39;building:levels&#39;, &#39;building&#39;, &#39;geometry&#39;, &#39;lat&#39;, &#39;long&#39;, &#39;brand&#39;, &#39;landuse&#39;,&#39;state&#39;, &#39;Key&#39;, &#39;Value&#39;, &#39;Group Name&#39;, &#39;Group&#39; ] . shops[columns].head().T . /opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . 0 1 2 3 4 . unique_id node/1466116159 | node/4829722523 | node/5240167118 | way/257720343 | way/257723588 | . osmid 1466116159 | 4829722523 | 5240167118 | 257720343 | 257723588 | . element_type node | node | node | way | way | . addr:city Yanakie | NaN | NaN | NaN | NaN | . addr:housenumber 3640 | NaN | NaN | NaN | NaN | . addr:postcode 3960 | NaN | NaN | NaN | NaN | . addr:street Meeniyan - Promontory Road | NaN | NaN | NaN | NaN | . postal_code NaN | NaN | NaN | NaN | NaN | . name Yanakie General Store | NaN | NaN | NaN | NaN | . opening_hours 08:00-18:00 | NaN | NaN | NaN | NaN | . operator Foodland Express | NaN | NaN | NaN | NaN | . phone +61 3 5687 1200 | NaN | NaN | NaN | NaN | . shop supermarket | supermarket | laundry | general | ticket | . source local knowledge | NaN | NaN | NaN | NaN | . building:levels NaN | NaN | NaN | 1 | NaN | . building NaN | NaN | NaN | NaN | yes | . geometry POINT (146.2076606 -38.8125072) | POINT (146.3209425 -39.0307625) | POINT (146.3178541 -39.0292552) | POLYGON ((146.3210142 -39.0310104, 146.3210156... | POLYGON ((146.249307 -38.858471, 146.2493843 -... | . brand NaN | NaN | NaN | NaN | NaN | . landuse NaN | NaN | NaN | NaN | NaN | . state NaN | NaN | NaN | NaN | NaN | . Key shop | shop | shop | shop | shop | . Value supermarket | supermarket | laundry | general | ticket | . Group Name General store, department store, mall | General store, department store, mall | Others | General store, department store, mall | Stationery, gifts, books, newspapers | . Group Mall | Mall | Others | Mall | Stationery | . shops.Group.value_counts() . /opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . Food 3742 Mall 1893 Vehicles 1885 Beauty 1231 Clothes 1190 Others 1008 Housewares 779 Stationery 605 Furniture 485 Health 459 Discount store 290 Electronics 273 Arts 253 Name: Group, dtype: int64 . shops[columns].to_file(f&quot;../data/toronto/raw/all_shops_victoria_australia.geojson&quot;, driver=&#39;GeoJSON&#39;) . shops[shops.geometry.geom_type.isin([&#39;Point&#39;])].shop.value_counts().head(60) . supermarket 917 convenience 801 hairdresser 661 clothes 649 bakery 547 alcohol 518 car_repair 495 car 268 beauty 259 butcher 230 furniture 186 department_store 182 laundry 177 bicycle 166 massage 157 greengrocer 151 gift 147 yes 142 car_parts 139 travel_agency 129 florist 128 books 127 newsagent 125 jewelry 118 wine 116 variety_store 105 doityourself 103 electronics 99 mobile_phone 96 optician 94 deli 90 outdoor 85 shoes 83 pet 79 computer 73 charity 72 art 71 houseware 68 dry_cleaning 67 sports 67 hardware 65 vacant 65 tyres 55 chemist 53 interior_decoration 51 seafood 51 garden_centre 49 tobacco 49 toys 45 stationery 41 lottery 41 copyshop 41 beverages 40 tattoo 40 funeral_directors 38 confectionery 35 antiques 35 pastry 33 second_hand 32 music 30 Name: shop, dtype: int64 . Explore Toronto . plc = &quot;Toronto, Golden Horseshoe, Ontario, Canada&quot; plot_shops(plc) . /opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . Explore The Greater Toronto and Hamilton Area (GTHA) . GTHA = {&quot;Toronto&quot;:1, &quot;Durham Region&quot;:2, &quot;York Region&quot;:3, &quot;Peel Region&quot;:4, &quot;Halton Region&quot;:5, &quot;Hamilton&quot;:6} GTHA . for region, code in GTHA.items(): print(region, code) plc = region + &quot;, Golden Horseshoe, Ontario, Canada&quot; plot_shops(plc) . Check shopping type in Ontaria, Canada . place = &quot;Ontario, Canada&quot; tags = {&quot;shop&quot;:True, &quot;opening_hours&quot;:&quot;*&quot;} can_shops = ox.geometries_from_place(place, tags) . can_shops.shape . (48557, 558) . columns = [&#39;unique_id&#39;, &#39;osmid&#39;, &#39;element_type&#39;, &#39;addr:city&#39;, &#39;addr:housenumber&#39;, &#39;addr:postcode&#39;, &#39;addr:street&#39;, &#39;postal_code&#39;, &#39;name&#39;, &#39;opening_hours&#39;, &#39;operator&#39;, &#39;phone&#39;, &#39;shop&#39;, &#39;source&#39;, &#39;building:levels&#39;, &#39;building&#39;, &#39;geometry&#39;, &#39;brand&#39;, &#39;landuse&#39;,&#39;state&#39;, &#39;Key&#39;, &#39;Value&#39;, &#39;Group Name&#39;, &#39;Group&#39; ] . can_shops[columns].to_file(f&quot;../data/toronto/raw/all_shops_ontario_canada.geojson&quot;, driver=&#39;GeoJSON&#39;) . can_shops = can_shops.merge(shop_types, how=&quot;left&quot;, left_on=&quot;shop&quot;, right_on=&quot;Value&quot;) can_shops.Group.value_counts() . Food 7592 Clothes 7390 Others 5996 Beauty 5725 Vehicles 5205 Mall 2699 Health 2322 Housewares 2194 Electronics 2178 Furniture 1905 Stationery 1470 Discount store 1271 Arts 1138 Name: Group, dtype: int64 . shops.Group.value_counts() . Food 3742 Mall 1893 Vehicles 1885 Beauty 1231 Clothes 1190 Others 1008 Housewares 779 Stationery 605 Furniture 485 Health 459 Discount store 290 Electronics 273 Arts 253 Name: Group, dtype: int64 .",
            "url": "https://danhphan.github.io/blog/2019/03/24/osm_data.html",
            "relUrl": "/2019/03/24/osm_data.html",
            "date": " • Mar 24, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "Scraping with Parsel and selenium",
            "content": "Parsel usage . text = u&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello, Parsel!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&quot; sl = Selector(text=text) sl.css(&#39;h1&#39;) . [&lt;Selector xpath=&#39;descendant-or-self::h1&#39; data=&#39;&lt;h1&gt;Hello, Parsel!&lt;/h1&gt;&#39;&gt;] . sl.xpath(&#39;//h1&#39;) . [&lt;Selector xpath=&#39;//h1&#39; data=&#39;&lt;h1&gt;Hello, Parsel!&lt;/h1&gt;&#39;&gt;] . sl.css(&#39;h1::text&#39;).get() . &#39;Hello, Parsel!&#39; . sl.xpath(&#39;//h1/text()&#39;).getall() . [&#39;Hello, Parsel!&#39;] . A look at CaffeF . url = &#39;https://s.cafef.vn/Lich-su-giao-dich-FPT-1.chn&#39; text = requests.get(url).text . cf = Selector(text) . rows = cf.xpath(&quot;//table[@id=&#39;GirdTable2&#39;]/tr[@id]&quot;) . for row in rows: date = row.xpath(&quot;td[@class=&#39;Item_DateItem&#39;]/text()&quot;).get() pr_scale = row.xpath(&quot;td[@class=&#39;Item_Price10&#39;][1]/text()&quot;).get() pr_close = row.xpath(&quot;td[@class=&#39;Item_Price10&#39;][2]/text()&quot;).get() print(date, pr_scale, pr_close) time.sleep(0.5) . 15/01/2021 66.60  66.60  14/01/2021 66.40  66.40  13/01/2021 66.50  66.50  12/01/2021 65.80  65.80  11/01/2021 64.70  64.70  08/01/2021 63.30  63.30  07/01/2021 62.50  62.50  06/01/2021 62.40  62.40  05/01/2021 62.70  62.70  04/01/2021 60.20  60.20  31/12/2020 59.10  59.10  30/12/2020 58.10  58.10  29/12/2020 58.60  58.60  28/12/2020 57.90  57.90  25/12/2020 57.60  57.60  24/12/2020 57.00  57.00  23/12/2020 57.50  57.50  22/12/2020 58.10  58.10  21/12/2020 57.50  57.50  18/12/2020 56.90  56.90  . A look at Vndirect . url = &#39;https://dstock.vndirect.com.vn/lich-su-gia/FPT&#39; text = requests.get(url).text vndr = Selector(text) . vndr . &lt;Selector xpath=None data=&#39;&lt;html&gt;&lt;head&gt;&lt;meta name=&#34;viewport&#34; con...&#39;&gt; . tbs = vndr.xpath(&quot;//table/tbody&quot;) . tbs . [&lt;Selector xpath=&#39;//table/tbody&#39; data=&#39;&lt;tbody&gt;&lt;/tbody&gt;&#39;&gt;, &lt;Selector xpath=&#39;//table/tbody&#39; data=&#39;&lt;tbody&gt;&lt;/tbody&gt;&#39;&gt;] . for tb in tbs: print(tb.xpath(&quot;/tr&quot;).get()) . None None . A look at BDS VN . urls = { &#39;All&#39; : &#39;https://batdongsan.com.vn/nha-dat-ban&#39;, &#39;Apartment&#39; : &#39;https://batdongsan.com.vn/ban-can-ho-chung-cu&#39;, &#39;All House&#39;: &#39;https://batdongsan.com.vn/ban-nha-dat&#39;, &#39;Normal House&#39; : &#39;https://batdongsan.com.vn/ban-nha-rieng&#39;, &#39;Villa House&#39; : &#39;https://batdongsan.com.vn/ban-nha-biet-thu-lien-ke&#39;, &#39;Biz House&#39; : &#39;https://batdongsan.com.vn/ban-nha-mat-pho&#39;, &#39;All Land&#39; : &#39;https://batdongsan.com.vn/ban-dat-dat-nen&#39;, &#39;Project Land&#39;: &#39;https://batdongsan.com.vn/ban-dat-nen-du-an&#39;, &#39;Normal Land&#39; : &#39;https://batdongsan.com.vn/ban-dat&#39;, &#39;Farm&#39; : &#39;https://batdongsan.com.vn/ban-trang-trai-khu-nghi-duong&#39;, &#39;Warehouse&#39; : &#39;https://batdongsan.com.vn/ban-kho-nha-xuong&#39;, &#39;Other&#39; : &#39;https://batdongsan.com.vn/ban-loai-bat-dong-san-khac&#39; } . date . &#39;18/12/2020&#39; . bdses = [] for cat, url in urls.items(): bds = Selector(requests.get(url).text) title = bds.xpath(&quot;//div/h1/text()&quot;).get() number = bds.xpath(&quot;//div/span[@id=&#39;count-number&#39;]/text()&quot;).get() bdses.append([cat, title, date, number]) print(cat, title, date, number) time.sleep(1) . All Mua bán nhà đất toàn quốc 18/12/2020 191,499 Apartment Bán căn hộ chung cư tại Việt Nam 18/12/2020 43,442 All House Nhà bán tại Việt Nam 18/12/2020 81,289 Normal House Bán nhà riêng tại Việt Nam 18/12/2020 44,433 Villa House Bán nhà biệt thự, liền kề tại Việt Nam 18/12/2020 14,450 Biz House Bán nhà mặt phố tại Việt Nam 18/12/2020 22,400 All Land Đất bán tại Việt Nam 18/12/2020 64,971 Project Land Bán đất nền dự án tại Việt Nam 18/12/2020 18,763 Normal Land Bán đất tại Việt Nam 18/12/2020 46,216 Farm Bán trang trại, khu nghỉ dưỡng tại Việt Nam 18/12/2020 424 Warehouse Bán kho, nhà xưởng tại Việt Nam 18/12/2020 551 Other Bán loại bất động sản khác tại Việt Nam 18/12/2020 830 . df = pd.DataFrame(bdses, columns=[&#39;Type&#39;,&#39;Desc&#39;,&#39;Date&#39;,&#39;Count&#39;]) df . Type Desc Date Count . 0 All | Mua bán nhà đất toàn quốc | 18/12/2020 | 191,499 | . 1 Apartment | Bán căn hộ chung cư tại Việt Nam | 18/12/2020 | 43,442 | . 2 All House | Nhà bán tại Việt Nam | 18/12/2020 | 81,289 | . 3 Normal House | Bán nhà riêng tại Việt Nam | 18/12/2020 | 44,433 | . 4 Villa House | Bán nhà biệt thự, liền kề tại Việt Nam | 18/12/2020 | 14,450 | . 5 Biz House | Bán nhà mặt phố tại Việt Nam | 18/12/2020 | 22,400 | . 6 All Land | Đất bán tại Việt Nam | 18/12/2020 | 64,971 | . 7 Project Land | Bán đất nền dự án tại Việt Nam | 18/12/2020 | 18,763 | . 8 Normal Land | Bán đất tại Việt Nam | 18/12/2020 | 46,216 | . 9 Farm | Bán trang trại, khu nghỉ dưỡng tại Việt Nam | 18/12/2020 | 424 | . 10 Warehouse | Bán kho, nhà xưởng tại Việt Nam | 18/12/2020 | 551 | . 11 Other | Bán loại bất động sản khác tại Việt Nam | 18/12/2020 | 830 | . Selinium . from selenium import webdriver from selenium.webdriver import * from selenium.webdriver.chrome.options import Options from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import time PATH = &#39;/home/danph/drivers/chromedriver&#39; def get_driver(url): chrome_options = Options() chrome_options.add_argument(&quot;--headless&quot;) driver = webdriver.Chrome( executable_path=PATH, options=chrome_options) driver.get(url) return driver def get_driver2(url): # Use headless option to not open a new browser window options = webdriver.ChromeOptions() options.add_argument(&quot;headless&quot;) desired_capabilities = options.to_capabilities() driver = webdriver.Chrome(executable_path=PATH, desired_capabilities=desired_capabilities) driver.get(url) return driver def get_table_results(driver): for row in driver.find_elements_by_xpath(&quot;//table[contains(@id,&#39;GirdTable&#39;)]//tr[@id]&quot;): time.sleep(0.5) print([cell.text for cell in row.find_elements_by_xpath(&quot;./td[position()&lt;=3]&quot;)]) . import time . time.strftime(&#39;%H:%M:%S&#39;), time.strftime(&#39;%Y-%M-%d&#39;), time.strftime(&#39;%Y-%M-%d_%H:%M:%S&#39;) . (&#39;15:30:42&#39;, &#39;2021-30-17&#39;, &#39;2021-30-17_15:30:42&#39;) . URL = &#39;https://s.cafef.vn/Lich-su-giao-dich-MVB-1.chn&#39; driver = get_driver(URL) . for row in driver.find_elements_by_xpath(&quot;//table[contains(@id,&#39;GirdTable&#39;)]//tr[@id]&quot;): time.sleep(0.5) print([cell.text for cell in row.find_elements_by_xpath(&quot;./td[position()&lt;=3]&quot;)]) . [&#39;15/01/2021&#39;, &#39;13.80 &#39;, &#39;13.80 &#39;] [&#39;14/01/2021&#39;, &#39;12.60 &#39;, &#39;12.60 &#39;] [&#39;13/01/2021&#39;, &#39;12.50 &#39;, &#39;12.50 &#39;] [&#39;12/01/2021&#39;, &#39;12.70 &#39;, &#39;12.70 &#39;] [&#39;11/01/2021&#39;, &#39;12.60 &#39;, &#39;12.60 &#39;] [&#39;08/01/2021&#39;, &#39;12.90 &#39;, &#39;12.90 &#39;] [&#39;07/01/2021&#39;, &#39;12.40 &#39;, &#39;12.40 &#39;] [&#39;06/01/2021&#39;, &#39;12.40 &#39;, &#39;12.40 &#39;] [&#39;05/01/2021&#39;, &#39;12.40 &#39;, &#39;12.40 &#39;] [&#39;04/01/2021&#39;, &#39;12.00 &#39;, &#39;12.00 &#39;] [&#39;31/12/2020&#39;, &#39;12.00 &#39;, &#39;12.00 &#39;] [&#39;30/12/2020&#39;, &#39;12.00 &#39;, &#39;12.00 &#39;] [&#39;29/12/2020&#39;, &#39;12.00 &#39;, &#39;12.00 &#39;] [&#39;28/12/2020&#39;, &#39;12.00 &#39;, &#39;12.00 &#39;] [&#39;25/12/2020&#39;, &#39;12.00 &#39;, &#39;12.00 &#39;] [&#39;24/12/2020&#39;, &#39;12.00 &#39;, &#39;12.00 &#39;] [&#39;23/12/2020&#39;, &#39;12.00 &#39;, &#39;12.00 &#39;] [&#39;22/12/2020&#39;, &#39;12.00 &#39;, &#39;12.00 &#39;] [&#39;21/12/2020&#39;, &#39;12.00 &#39;, &#39;12.00 &#39;] [&#39;18/12/2020&#39;, &#39;11.60 &#39;, &#39;11.60 &#39;] . while True: try: page_number = driver.find_element_by_xpath(&quot;//table[@class=&#39;CafeF_Paging&#39;]//td/span&quot;).text print(&quot;Page #&quot; + page_number) get_table_results(driver) next_link = driver.find_element_by_xpath(&quot;//table[@class=&#39;CafeF_Paging&#39;]//a[contains(@title,&#39;Next to Page&#39;)]&quot;) next_link.click() time.sleep(10) #WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH,&quot;//table[@id=&#39;GirdTable2&#39;]&quot;))) except Exception: print(Exception) break . Page #1 [&#39;22/12/2020&#39;, &#39;29.50 &#39;, &#39;29.50 &#39;] [&#39;21/12/2020&#39;, &#39;30.20 &#39;, &#39;30.20 &#39;] [&#39;18/12/2020&#39;, &#39;31.05 &#39;, &#39;31.05 &#39;] [&#39;17/12/2020&#39;, &#39;30.90 &#39;, &#39;30.90 &#39;] [&#39;16/12/2020&#39;, &#39;31.20 &#39;, &#39;31.20 &#39;] [&#39;15/12/2020&#39;, &#39;29.80 &#39;, &#39;29.80 &#39;] [&#39;14/12/2020&#39;, &#39;29.60 &#39;, &#39;29.60 &#39;] [&#39;11/12/2020&#39;, &#39;29.50 &#39;, &#39;29.50 &#39;] [&#39;10/12/2020&#39;, &#39;29.65 &#39;, &#39;29.65 &#39;] [&#39;09/12/2020&#39;, &#39;30.40 &#39;, &#39;30.40 &#39;] [&#39;08/12/2020&#39;, &#39;30.30 &#39;, &#39;30.30 &#39;] [&#39;07/12/2020&#39;, &#39;30.60 &#39;, &#39;30.60 &#39;] [&#39;04/12/2020&#39;, &#39;30.35 &#39;, &#39;30.35 &#39;] [&#39;03/12/2020&#39;, &#39;30.80 &#39;, &#39;30.80 &#39;] [&#39;02/12/2020&#39;, &#39;30.80 &#39;, &#39;30.80 &#39;] [&#39;01/12/2020&#39;, &#39;30.45 &#39;, &#39;30.45 &#39;] [&#39;30/11/2020&#39;, &#39;30.30 &#39;, &#39;30.30 &#39;] [&#39;27/11/2020&#39;, &#39;29.70 &#39;, &#39;29.70 &#39;] [&#39;26/11/2020&#39;, &#39;29.10 &#39;, &#39;29.10 &#39;] [&#39;25/11/2020&#39;, &#39;28.60 &#39;, &#39;28.60 &#39;] Page #2 [&#39;23/11/2020&#39;, &#39;28.70 &#39;, &#39;28.70 &#39;] [&#39;20/11/2020&#39;, &#39;28.60 &#39;, &#39;28.60 &#39;] [&#39;19/11/2020&#39;, &#39;28.50 &#39;, &#39;28.50 &#39;] [&#39;18/11/2020&#39;, &#39;28.80 &#39;, &#39;28.80 &#39;] [&#39;17/11/2020&#39;, &#39;29.00 &#39;, &#39;29.00 &#39;] [&#39;16/11/2020&#39;, &#39;28.80 &#39;, &#39;28.80 &#39;] [&#39;13/11/2020&#39;, &#39;29.10 &#39;, &#39;29.10 &#39;] [&#39;12/11/2020&#39;, &#39;29.10 &#39;, &#39;29.10 &#39;] [&#39;11/11/2020&#39;, &#39;29.30 &#39;, &#39;29.30 &#39;] [&#39;10/11/2020&#39;, &#39;29.50 &#39;, &#39;29.50 &#39;] [&#39;09/11/2020&#39;, &#39;29.60 &#39;, &#39;29.60 &#39;] [&#39;06/11/2020&#39;, &#39;29.20 &#39;, &#39;29.20 &#39;] [&#39;05/11/2020&#39;, &#39;29.40 &#39;, &#39;29.40 &#39;] [&#39;04/11/2020&#39;, &#39;29.45 &#39;, &#39;29.45 &#39;] [&#39;03/11/2020&#39;, &#39;29.30 &#39;, &#39;29.30 &#39;] [&#39;02/11/2020&#39;, &#39;28.25 &#39;, &#39;28.25 &#39;] [&#39;30/10/2020&#39;, &#39;28.10 &#39;, &#39;28.10 &#39;] [&#39;29/10/2020&#39;, &#39;28.00 &#39;, &#39;28.00 &#39;] [&#39;28/10/2020&#39;, &#39;28.20 &#39;, &#39;28.20 &#39;] [&#39;27/10/2020&#39;, &#39;28.70 &#39;, &#39;28.70 &#39;] Page #3 [&#39;26/10/2020&#39;, &#39;29.00 &#39;, &#39;29.00 &#39;] [&#39;23/10/2020&#39;, &#39;29.30 &#39;, &#39;29.30 &#39;] [&#39;22/10/2020&#39;, &#39;29.30 &#39;, &#39;29.30 &#39;] [&#39;21/10/2020&#39;, &#39;29.20 &#39;, &#39;29.20 &#39;] [&#39;20/10/2020&#39;, &#39;29.20 &#39;, &#39;29.20 &#39;] [&#39;19/10/2020&#39;, &#39;29.30 &#39;, &#39;29.30 &#39;] [&#39;16/10/2020&#39;, &#39;29.50 &#39;, &#39;29.50 &#39;] [&#39;15/10/2020&#39;, &#39;29.90 &#39;, &#39;29.90 &#39;] [&#39;14/10/2020&#39;, &#39;30.00 &#39;, &#39;30.00 &#39;] [&#39;13/10/2020&#39;, &#39;30.20 &#39;, &#39;30.20 &#39;] [&#39;12/10/2020&#39;, &#39;30.00 &#39;, &#39;30.00 &#39;] [&#39;09/10/2020&#39;, &#39;30.35 &#39;, &#39;30.35 &#39;] [&#39;08/10/2020&#39;, &#39;30.35 &#39;, &#39;30.35 &#39;] [&#39;07/10/2020&#39;, &#39;30.25 &#39;, &#39;30.25 &#39;] [&#39;06/10/2020&#39;, &#39;30.45 &#39;, &#39;30.45 &#39;] [&#39;05/10/2020&#39;, &#39;30.35 &#39;, &#39;30.35 &#39;] [&#39;02/10/2020&#39;, &#39;30.05 &#39;, &#39;30.05 &#39;] [&#39;01/10/2020&#39;, &#39;30.70 &#39;, &#39;30.70 &#39;] [&#39;30/09/2020&#39;, &#39;29.45 &#39;, &#39;29.45 &#39;] [&#39;29/09/2020&#39;, &#39;29.50 &#39;, &#39;29.50 &#39;] Page #4 [&#39;28/09/2020&#39;, &#39;29.70 &#39;, &#39;29.70 &#39;] [&#39;25/09/2020&#39;, &#39;29.75 &#39;, &#39;29.75 &#39;] [&#39;24/09/2020&#39;, &#39;29.60 &#39;, &#39;29.60 &#39;] [&#39;23/09/2020&#39;, &#39;29.80 &#39;, &#39;29.80 &#39;] [&#39;22/09/2020&#39;, &#39;29.55 &#39;, &#39;29.55 &#39;] [&#39;21/09/2020&#39;, &#39;29.50 &#39;, &#39;29.50 &#39;] [&#39;18/09/2020&#39;, &#39;29.50 &#39;, &#39;29.50 &#39;] [&#39;17/09/2020&#39;, &#39;29.25 &#39;, &#39;29.25 &#39;] [&#39;16/09/2020&#39;, &#39;29.65 &#39;, &#39;29.65 &#39;] [&#39;15/09/2020&#39;, &#39;29.80 &#39;, &#39;29.80 &#39;] [&#39;14/09/2020&#39;, &#39;30.05 &#39;, &#39;30.05 &#39;] [&#39;11/09/2020&#39;, &#39;29.60 &#39;, &#39;29.60 &#39;] [&#39;10/09/2020&#39;, &#39;29.75 &#39;, &#39;29.75 &#39;] [&#39;09/09/2020&#39;, &#39;29.50 &#39;, &#39;29.50 &#39;] [&#39;08/09/2020&#39;, &#39;28.65 &#39;, &#39;28.65 &#39;] [&#39;07/09/2020&#39;, &#39;28.40 &#39;, &#39;28.40 &#39;] [&#39;04/09/2020&#39;, &#39;28.25 &#39;, &#39;28.25 &#39;] [&#39;03/09/2020&#39;, &#39;28.05 &#39;, &#39;28.05 &#39;] [&#39;01/09/2020&#39;, &#39;28.45 &#39;, &#39;28.45 &#39;] [&#39;31/08/2020&#39;, &#39;27.85 &#39;, &#39;27.85 &#39;] Page #5 [&#39;28/08/2020&#39;, &#39;28.15 &#39;, &#39;28.15 &#39;] [&#39;27/08/2020&#39;, &#39;27.15 &#39;, &#39;27.15 &#39;] [&#39;26/08/2020&#39;, &#39;25.73 &#39;, &#39;28.30 &#39;] [&#39;25/08/2020&#39;, &#39;25.68 &#39;, &#39;28.25 &#39;] [&#39;24/08/2020&#39;, &#39;25.86 &#39;, &#39;28.45 &#39;] [&#39;21/08/2020&#39;, &#39;25.73 &#39;, &#39;28.30 &#39;] [&#39;20/08/2020&#39;, &#39;25.50 &#39;, &#39;28.05 &#39;] [&#39;19/08/2020&#39;, &#39;25.64 &#39;, &#39;28.20 &#39;] [&#39;18/08/2020&#39;, &#39;25.91 &#39;, &#39;28.50 &#39;] [&#39;17/08/2020&#39;, &#39;25.82 &#39;, &#39;28.40 &#39;] [&#39;14/08/2020&#39;, &#39;25.91 &#39;, &#39;28.50 &#39;] [&#39;13/08/2020&#39;, &#39;26.00 &#39;, &#39;28.60 &#39;] [&#39;12/08/2020&#39;, &#39;25.55 &#39;, &#39;28.10 &#39;] [&#39;11/08/2020&#39;, &#39;24.91 &#39;, &#39;27.40 &#39;] [&#39;10/08/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] [&#39;07/08/2020&#39;, &#39;24.41 &#39;, &#39;26.85 &#39;] [&#39;06/08/2020&#39;, &#39;24.36 &#39;, &#39;26.80 &#39;] [&#39;05/08/2020&#39;, &#39;24.27 &#39;, &#39;26.70 &#39;] [&#39;04/08/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] [&#39;03/08/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] Page #6 [&#39;31/07/2020&#39;, &#39;24.00 &#39;, &#39;26.40 &#39;] [&#39;30/07/2020&#39;, &#39;23.91 &#39;, &#39;26.30 &#39;] [&#39;29/07/2020&#39;, &#39;24.36 &#39;, &#39;26.80 &#39;] [&#39;28/07/2020&#39;, &#39;23.91 &#39;, &#39;26.30 &#39;] [&#39;27/07/2020&#39;, &#39;23.64 &#39;, &#39;26.00 &#39;] [&#39;24/07/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] [&#39;23/07/2020&#39;, &#39;24.45 &#39;, &#39;26.90 &#39;] [&#39;22/07/2020&#39;, &#39;24.36 &#39;, &#39;26.80 &#39;] [&#39;21/07/2020&#39;, &#39;24.23 &#39;, &#39;26.65 &#39;] [&#39;20/07/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] [&#39;17/07/2020&#39;, &#39;24.18 &#39;, &#39;26.60 &#39;] [&#39;16/07/2020&#39;, &#39;24.27 &#39;, &#39;26.70 &#39;] [&#39;15/07/2020&#39;, &#39;24.18 &#39;, &#39;26.60 &#39;] [&#39;14/07/2020&#39;, &#39;24.00 &#39;, &#39;26.40 &#39;] [&#39;13/07/2020&#39;, &#39;24.00 &#39;, &#39;26.40 &#39;] [&#39;10/07/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] [&#39;09/07/2020&#39;, &#39;24.27 &#39;, &#39;26.70 &#39;] [&#39;08/07/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] [&#39;07/07/2020&#39;, &#39;24.18 &#39;, &#39;26.60 &#39;] [&#39;06/07/2020&#39;, &#39;24.55 &#39;, &#39;27.00 &#39;] Page #7 [&#39;03/07/2020&#39;, &#39;24.45 &#39;, &#39;26.90 &#39;] [&#39;02/07/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;01/07/2020&#39;, &#39;24.82 &#39;, &#39;27.30 &#39;] [&#39;30/06/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;29/06/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;26/06/2020&#39;, &#39;25.27 &#39;, &#39;27.80 &#39;] [&#39;25/06/2020&#39;, &#39;25.27 &#39;, &#39;27.80 &#39;] [&#39;24/06/2020&#39;, &#39;25.55 &#39;, &#39;28.10 &#39;] [&#39;23/06/2020&#39;, &#39;24.95 &#39;, &#39;27.45 &#39;] [&#39;22/06/2020&#39;, &#39;24.91 &#39;, &#39;27.40 &#39;] [&#39;19/06/2020&#39;, &#39;24.91 &#39;, &#39;27.40 &#39;] [&#39;18/06/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;17/06/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;16/06/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;15/06/2020&#39;, &#39;24.45 &#39;, &#39;26.90 &#39;] [&#39;12/06/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;11/06/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;10/06/2020&#39;, &#39;24.82 &#39;, &#39;27.30 &#39;] [&#39;09/06/2020&#39;, &#39;24.82 &#39;, &#39;27.30 &#39;] [&#39;08/06/2020&#39;, &#39;25.00 &#39;, &#39;27.50 &#39;] Page #8 [&#39;05/06/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;04/06/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;03/06/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;02/06/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;01/06/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;29/05/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;28/05/2020&#39;, &#39;24.77 &#39;, &#39;27.25 &#39;] [&#39;27/05/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;26/05/2020&#39;, &#39;24.82 &#39;, &#39;27.30 &#39;] [&#39;25/05/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;22/05/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;21/05/2020&#39;, &#39;25.00 &#39;, &#39;27.50 &#39;] [&#39;20/05/2020&#39;, &#39;24.91 &#39;, &#39;27.40 &#39;] [&#39;19/05/2020&#39;, &#39;24.91 &#39;, &#39;27.40 &#39;] [&#39;18/05/2020&#39;, &#39;24.86 &#39;, &#39;27.35 &#39;] [&#39;15/05/2020&#39;, &#39;24.86 &#39;, &#39;27.35 &#39;] [&#39;14/05/2020&#39;, &#39;24.82 &#39;, &#39;27.30 &#39;] [&#39;13/05/2020&#39;, &#39;24.82 &#39;, &#39;27.30 &#39;] [&#39;12/05/2020&#39;, &#39;24.91 &#39;, &#39;27.40 &#39;] [&#39;11/05/2020&#39;, &#39;24.91 &#39;, &#39;27.40 &#39;] Page #9 [&#39;08/05/2020&#39;, &#39;24.68 &#39;, &#39;27.15 &#39;] [&#39;07/05/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;06/05/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;05/05/2020&#39;, &#39;24.82 &#39;, &#39;27.30 &#39;] [&#39;04/05/2020&#39;, &#39;24.82 &#39;, &#39;27.30 &#39;] [&#39;29/04/2020&#39;, &#39;25.27 &#39;, &#39;27.80 &#39;] [&#39;28/04/2020&#39;, &#39;23.95 &#39;, &#39;26.35 &#39;] [&#39;27/04/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] [&#39;24/04/2020&#39;, &#39;24.27 &#39;, &#39;26.70 &#39;] [&#39;23/04/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;22/04/2020&#39;, &#39;24.91 &#39;, &#39;27.40 &#39;] [&#39;21/04/2020&#39;, &#39;25.09 &#39;, &#39;27.60 &#39;] [&#39;20/04/2020&#39;, &#39;25.50 &#39;, &#39;28.05 &#39;] [&#39;17/04/2020&#39;, &#39;25.64 &#39;, &#39;28.20 &#39;] [&#39;16/04/2020&#39;, &#39;25.36 &#39;, &#39;27.90 &#39;] [&#39;15/04/2020&#39;, &#39;26.18 &#39;, &#39;28.80 &#39;] [&#39;14/04/2020&#39;, &#39;26.36 &#39;, &#39;29.00 &#39;] [&#39;13/04/2020&#39;, &#39;26.82 &#39;, &#39;29.50 &#39;] [&#39;10/04/2020&#39;, &#39;25.73 &#39;, &#39;28.30 &#39;] [&#39;09/04/2020&#39;, &#39;25.91 &#39;, &#39;28.50 &#39;] Page #10 [&#39;08/04/2020&#39;, &#39;26.05 &#39;, &#39;28.65 &#39;] [&#39;07/04/2020&#39;, &#39;25.82 &#39;, &#39;28.40 &#39;] [&#39;06/04/2020&#39;, &#39;25.64 &#39;, &#39;28.20 &#39;] [&#39;03/04/2020&#39;, &#39;25.45 &#39;, &#39;28.00 &#39;] [&#39;01/04/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] [&#39;31/03/2020&#39;, &#39;23.91 &#39;, &#39;26.30 &#39;] [&#39;30/03/2020&#39;, &#39;23.95 &#39;, &#39;26.35 &#39;] [&#39;27/03/2020&#39;, &#39;24.41 &#39;, &#39;26.85 &#39;] [&#39;26/03/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;25/03/2020&#39;, &#39;24.73 &#39;, &#39;27.20 &#39;] [&#39;24/03/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] [&#39;23/03/2020&#39;, &#39;23.73 &#39;, &#39;26.10 &#39;] [&#39;20/03/2020&#39;, &#39;24.05 &#39;, &#39;26.45 &#39;] [&#39;19/03/2020&#39;, &#39;23.73 &#39;, &#39;26.10 &#39;] [&#39;18/03/2020&#39;, &#39;23.82 &#39;, &#39;26.20 &#39;] [&#39;17/03/2020&#39;, &#39;23.86 &#39;, &#39;26.25 &#39;] [&#39;16/03/2020&#39;, &#39;24.18 &#39;, &#39;26.60 &#39;] [&#39;13/03/2020&#39;, &#39;24.18 &#39;, &#39;26.60 &#39;] [&#39;12/03/2020&#39;, &#39;24.09 &#39;, &#39;26.50 &#39;] [&#39;11/03/2020&#39;, &#39;24.14 &#39;, &#39;26.55 &#39;] Page #11 [&#39;10/03/2020&#39;, &#39;24.64 &#39;, &#39;27.10 &#39;] [&#39;09/03/2020&#39;, &#39;24.59 &#39;, &#39;27.05 &#39;] [&#39;06/03/2020&#39;, &#39;25.27 &#39;, &#39;27.80 &#39;] [&#39;05/03/2020&#39;, &#39;25.32 &#39;, &#39;27.85 &#39;] [&#39;04/03/2020&#39;, &#39;25.45 &#39;, &#39;28.00 &#39;] [&#39;03/03/2020&#39;, &#39;25.41 &#39;, &#39;27.95 &#39;] [&#39;02/03/2020&#39;, &#39;25.18 &#39;, &#39;27.70 &#39;] [&#39;28/02/2020&#39;, &#39;24.95 &#39;, &#39;27.45 &#39;] [&#39;27/02/2020&#39;, &#39;25.77 &#39;, &#39;28.35 &#39;] [&#39;26/02/2020&#39;, &#39;25.95 &#39;, &#39;28.55 &#39;] [&#39;25/02/2020&#39;, &#39;26.14 &#39;, &#39;28.75 &#39;] [&#39;24/02/2020&#39;, &#39;26.09 &#39;, &#39;28.70 &#39;] [&#39;21/02/2020&#39;, &#39;26.18 &#39;, &#39;28.80 &#39;] [&#39;20/02/2020&#39;, &#39;26.18 &#39;, &#39;28.80 &#39;] [&#39;19/02/2020&#39;, &#39;26.27 &#39;, &#39;28.90 &#39;] [&#39;18/02/2020&#39;, &#39;26.23 &#39;, &#39;28.85 &#39;] [&#39;17/02/2020&#39;, &#39;26.36 &#39;, &#39;29.00 &#39;] [&#39;14/02/2020&#39;, &#39;26.23 &#39;, &#39;28.85 &#39;] [&#39;13/02/2020&#39;, &#39;26.32 &#39;, &#39;28.95 &#39;] [&#39;12/02/2020&#39;, &#39;26.41 &#39;, &#39;29.05 &#39;] Page #12 [&#39;11/02/2020&#39;, &#39;26.64 &#39;, &#39;29.30 &#39;] [&#39;10/02/2020&#39;, &#39;26.86 &#39;, &#39;29.55 &#39;] [&#39;07/02/2020&#39;, &#39;26.77 &#39;, &#39;29.45 &#39;] [&#39;06/02/2020&#39;, &#39;26.95 &#39;, &#39;29.65 &#39;] [&#39;05/02/2020&#39;, &#39;26.91 &#39;, &#39;29.60 &#39;] [&#39;04/02/2020&#39;, &#39;27.00 &#39;, &#39;29.70 &#39;] [&#39;03/02/2020&#39;, &#39;26.82 &#39;, &#39;29.50 &#39;] [&#39;31/01/2020&#39;, &#39;27.05 &#39;, &#39;29.75 &#39;] [&#39;30/01/2020&#39;, &#39;27.45 &#39;, &#39;30.20 &#39;] [&#39;22/01/2020&#39;, &#39;27.36 &#39;, &#39;30.10 &#39;] [&#39;21/01/2020&#39;, &#39;27.09 &#39;, &#39;29.80 &#39;] [&#39;20/01/2020&#39;, &#39;26.95 &#39;, &#39;29.65 &#39;] [&#39;17/01/2020&#39;, &#39;27.00 &#39;, &#39;29.70 &#39;] [&#39;16/01/2020&#39;, &#39;27.14 &#39;, &#39;29.85 &#39;] [&#39;15/01/2020&#39;, &#39;26.95 &#39;, &#39;29.65 &#39;] [&#39;14/01/2020&#39;, &#39;26.82 &#39;, &#39;29.50 &#39;] [&#39;13/01/2020&#39;, &#39;27.41 &#39;, &#39;30.15 &#39;] [&#39;10/01/2020&#39;, &#39;27.64 &#39;, &#39;30.40 &#39;] [&#39;09/01/2020&#39;, &#39;27.27 &#39;, &#39;30.00 &#39;] &lt;class &#39;Exception&#39;&gt; .",
            "url": "https://danhphan.github.io/blog/jupyter/parsel/selenium/2019/01/10/pyscraper.html",
            "relUrl": "/jupyter/parsel/selenium/2019/01/10/pyscraper.html",
            "date": " • Jan 10, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "Spatial analysis with geopandas",
            "content": "import geopandas as gpd . Indexing and Selecting Data . world = gpd.read_file(gpd.datasets.get_path(&quot;naturalearth_lowres&quot;)) . type(world), world.shape . (geopandas.geodataframe.GeoDataFrame, (177, 6)) . world.head() . pop_est continent name iso_a3 gdp_md_est geometry . 0 920938 | Oceania | Fiji | FJI | 8374.0 | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | . 1 53950935 | Africa | Tanzania | TZA | 150600.0 | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | . 2 603253 | Africa | W. Sahara | ESH | 906.5 | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | . 3 35623680 | North America | Canada | CAN | 1674000.0 | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 326625791 | North America | United States of America | USA | 18560000.0 | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . world.continent.value_counts() . Africa 51 Asia 47 Europe 39 North America 18 South America 13 Oceania 7 Antarctica 1 Seven seas (open ocean) 1 Name: continent, dtype: int64 . world.crs . &lt;Geographic 2D CRS: EPSG:4326&gt; Name: WGS 84 Axis Info [ellipsoidal]: - Lat[north]: Geodetic latitude (degree) - Lon[east]: Geodetic longitude (degree) Area of Use: - name: World. - bounds: (-180.0, -90.0, 180.0, 90.0) Datum: World Geodetic System 1984 - Ellipsoid: WGS 84 - Prime Meridian: Greenwich . southern_world = world.cx[:, :0] . southern_world.plot(figsize=(10,6)) . &lt;AxesSubplot:&gt; . southern_world.crs . &lt;Geographic 2D CRS: EPSG:4326&gt; Name: WGS 84 Axis Info [ellipsoidal]: - Lat[north]: Geodetic latitude (degree) - Lon[east]: Geodetic longitude (degree) Area of Use: - name: World. - bounds: (-180.0, -90.0, 180.0, 90.0) Datum: World Geodetic System 1984 - Ellipsoid: WGS 84 - Prime Meridian: Greenwich . Mapping . world = gpd.read_file(gpd.datasets.get_path(&quot;naturalearth_lowres&quot;)) . cities = gpd.read_file(gpd.datasets.get_path(&quot;naturalearth_cities&quot;)) . world.head() . pop_est continent name iso_a3 gdp_md_est geometry . 0 920938 | Oceania | Fiji | FJI | 8374.0 | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | . 1 53950935 | Africa | Tanzania | TZA | 150600.0 | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | . 2 603253 | Africa | W. Sahara | ESH | 906.5 | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | . 3 35623680 | North America | Canada | CAN | 1674000.0 | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 326625791 | North America | United States of America | USA | 18560000.0 | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . world.plot() . &lt;AxesSubplot:&gt; . cities.head() . name geometry . 0 Vatican City | POINT (12.45339 41.90328) | . 1 San Marino | POINT (12.44177 43.93610) | . 2 Vaduz | POINT (9.51667 47.13372) | . 3 Luxembourg | POINT (6.13000 49.61166) | . 4 Palikir | POINT (158.14997 6.91664) | . cities.plot() . &lt;AxesSubplot:&gt; . ax = world.plot() . cities.plot(ax=ax) . &lt;AxesSubplot:&gt; . &lt;Figure size 432x288 with 0 Axes&gt; . Choropleth Maps . world = world[(world.pop_est&gt;0) &amp; (world.name!=&quot;Antarctica&quot;)] . world.shape . (176, 6) . world[&#39;gdp_per_cap&#39;] = world.gdp_md_est/world.pop_est . world.plot(column=&quot;gdp_per_cap&quot;, figsize=(10,6)) . &lt;AxesSubplot:&gt; . Creating a legend . from mpl_toolkits.axes_grid1 import make_axes_locatable fig, ax = plt.subplots(1, 1) divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.1) world.plot(column=&#39;pop_est&#39;, ax=ax, legend=True, cax=cax, figsize=(10,6)) . &lt;AxesSubplot:&gt; . world.boundary.plot() . &lt;AxesSubplot:&gt; . Maps with Layers . cities.plot(marker=&quot;*&quot;, color=&quot;green&quot;, markersize=5) . &lt;AxesSubplot:&gt; . cities.crs, world.crs . (&lt;Geographic 2D CRS: EPSG:4326&gt; Name: WGS 84 Axis Info [ellipsoidal]: - Lat[north]: Geodetic latitude (degree) - Lon[east]: Geodetic longitude (degree) Area of Use: - name: World. - bounds: (-180.0, -90.0, 180.0, 90.0) Datum: World Geodetic System 1984 - Ellipsoid: WGS 84 - Prime Meridian: Greenwich, &lt;Geographic 2D CRS: EPSG:4326&gt; Name: WGS 84 Axis Info [ellipsoidal]: - Lat[north]: Geodetic latitude (degree) - Lon[east]: Geodetic longitude (degree) Area of Use: - name: World. - bounds: (-180.0, -90.0, 180.0, 90.0) Datum: World Geodetic System 1984 - Ellipsoid: WGS 84 - Prime Meridian: Greenwich) . cities = cities.to_crs(world.crs) . Plot layers: option 1 . base = world.plot(color=&quot;white&quot;, edgecolor=&quot;black&quot;) cities.plot(ax=base, marker=&quot;o&quot;, color=&quot;red&quot;, markersize=5) . &lt;AxesSubplot:&gt; . Plot layers: option 2 . import matplotlib.pyplot as plt . fig, ax = plt.subplots() ax.set_aspect(&quot;equal&quot;) world.plot(ax=ax, color=&quot;white&quot;, edgecolor=&quot;black&quot;) cities.plot(ax=ax, marker=&quot;o&quot;, color=&quot;red&quot;, markersize=5) plt.show() .",
            "url": "https://danhphan.github.io/blog/geopandas/spatial/2018/06/14/geopandas.html",
            "relUrl": "/geopandas/spatial/2018/06/14/geopandas.html",
            "date": " • Jun 14, 2018"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://danhphan.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}